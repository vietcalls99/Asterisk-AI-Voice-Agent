# Asterisk AI Voice Agent - Main Services
#
# DEFAULT NETWORK MODE: Host (for telephony/low-latency)
# - Direct access to host network (127.0.0.1 reaches Asterisk)
# - No port mapping needed
# - Best for telephony integrations
#
# PERMISSION ALIGNMENT:
# If your Asterisk uses a different GID than 995 (FreePBX default):
#   export ASTERISK_GID=$(id -g asterisk)
#   docker compose build ai-engine
#   docker compose up -d
#
# Prometheus-format metrics are available at `http://<host>:15000/metrics`.
# Per-call debugging is via Admin UI â†’ Call History.

services:
  ai-engine:
    image: asterisk-ai-voice-agent-ai-engine:latest
    pull_policy: build
    build:
      context: .
      dockerfile: Dockerfile
      args:
        ASTERISK_GID: ${ASTERISK_GID:-995}
    container_name: ai_engine
    # Container runs as appuser (member of asterisk group via Dockerfile)
    # Files inherit group ownership from setgid directory (set up by preflight.sh)
    user: "appuser"
    network_mode: host
    volumes:
      - ./src:/app/src
      - ./main.py:/app/main.py
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ./models:/app/models
      - ./asterisk_media:/mnt/asterisk_media
      - ./data:/app/data
    env_file:
      - .env
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      # Timezone for consistent timestamp display and call history records
      # Configurable via .env (default: America/Phoenix)
      - TZ=${TZ:-America/Phoenix}
      # Enable health endpoint access from other containers (admin-ui needs /sessions/stats)
      - HEALTH_BIND_HOST=0.0.0.0
      # Note: ASTERISK_HOST is loaded from .env via env_file directive above
      # Do NOT add ASTERISK_HOST here - it prevents UI env changes from taking effect
    tty: true
    stdin_open: true
    restart: unless-stopped

  local-ai-server:
    image: asterisk-ai-voice-agent-local-ai-server:latest
    pull_policy: build
    build:
      context: ./local_ai_server
      dockerfile: Dockerfile
      args:
        - INCLUDE_KROKO_EMBEDDED=${INCLUDE_KROKO_EMBEDDED:-false}
        - INCLUDE_FASTER_WHISPER=${INCLUDE_FASTER_WHISPER:-false}
        - INCLUDE_WHISPER_CPP=${INCLUDE_WHISPER_CPP:-false}
        - INCLUDE_MELOTTS=${INCLUDE_MELOTTS:-false}
    container_name: local_ai_server
    network_mode: host
    env_file:
      - .env
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - LOCAL_LOG_LEVEL=${LOCAL_LOG_LEVEL:-INFO}
      - LOCAL_DEBUG=${LOCAL_DEBUG:-0}
      # STT Configuration
      - LOCAL_STT_BACKEND=${LOCAL_STT_BACKEND:-vosk}
      - LOCAL_STT_MODEL_PATH=${LOCAL_STT_MODEL_PATH:-/app/models/stt/vosk-model-en-us-0.22}
      - LOCAL_STT_IDLE_MS=${LOCAL_STT_IDLE_MS:-5000}
      # LLM Configuration
      - LOCAL_LLM_MODEL_PATH=${LOCAL_LLM_MODEL_PATH:-/app/models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf}
      - LOCAL_LLM_THREADS=${LOCAL_LLM_THREADS:-16}
      - LOCAL_LLM_CONTEXT=${LOCAL_LLM_CONTEXT:-768}
      - LOCAL_LLM_BATCH=${LOCAL_LLM_BATCH:-128}
      - LOCAL_LLM_MAX_TOKENS=${LOCAL_LLM_MAX_TOKENS:-64}
      - LOCAL_LLM_TEMPERATURE=${LOCAL_LLM_TEMPERATURE:-0.4}
      - LOCAL_LLM_TOP_P=${LOCAL_LLM_TOP_P:-0.85}
      - LOCAL_LLM_REPEAT_PENALTY=${LOCAL_LLM_REPEAT_PENALTY:-1.05}
      - LOCAL_LLM_USE_MLOCK=${LOCAL_LLM_USE_MLOCK:-0}
      - LOCAL_LLM_INFER_TIMEOUT_SEC=${LOCAL_LLM_INFER_TIMEOUT_SEC:-30}
      # GPU: 0=CPU only, -1=auto-detect, N=specific layers
      - LOCAL_LLM_GPU_LAYERS=${LOCAL_LLM_GPU_LAYERS:-0}
      # TTS Configuration
      - LOCAL_TTS_BACKEND=${LOCAL_TTS_BACKEND:-piper}
      - LOCAL_TTS_MODEL_PATH=${LOCAL_TTS_MODEL_PATH:-/app/models/tts/en_US-lessac-medium.onnx}
      - KOKORO_MODE=${KOKORO_MODE:-local}
      - KOKORO_API_BASE_URL=${KOKORO_API_BASE_URL:-https://voice-generator.pages.dev/api/v1}
      - KOKORO_API_KEY=${KOKORO_API_KEY:-}
      - KOKORO_API_MODEL=${KOKORO_API_MODEL:-model}
      - KOKORO_VOICE=${KOKORO_VOICE:-af_heart}
      - LOCAL_LLM_SYSTEM_PROMPT=${LOCAL_LLM_SYSTEM_PROMPT:-You are a helpful AI voice assistant. When the caller wants to end the call or says goodbye, output <tool_call>{"name":"hangup_call","arguments":{"farewell":"Goodbye, have a great day!"}}</tool_call> and say a brief farewell. When the caller asks to email the transcript, output <tool_call>{"name":"request_transcript","arguments":{"email":"caller@example.com"}}</tool_call>. Always provide a spoken response.}
    tty: true
    stdin_open: true
    restart: unless-stopped
    # Uncomment below for NVIDIA GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    healthcheck:
      test:
        - CMD-SHELL
        - |
          python - <<'PY'
          import asyncio
          import json
          import os
          import websockets

          async def main():
              port = os.getenv("LOCAL_WS_PORT", "8765")
              token = (os.getenv("LOCAL_WS_AUTH_TOKEN", "") or "").strip()
              uri = f"ws://127.0.0.1:{port}"

              ws = await websockets.connect(
                  uri,
                  ping_interval=None,
                  ping_timeout=None,
                  close_timeout=2,
                  max_size=None,
              )
              try:
                  if token:
                      await ws.send(json.dumps({"type": "auth", "auth_token": token}))
                      raw = await ws.recv()
                      if isinstance(raw, (bytes, bytearray)):
                          raise RuntimeError("Unexpected binary auth response")
                      data = json.loads(raw)
                      if data.get("type") != "auth_response" or data.get("status") != "ok":
                          raise RuntimeError(f"Auth rejected: {data}")

                  await ws.send(json.dumps({"type": "status"}))
                  raw = await ws.recv()
                  if isinstance(raw, (bytes, bytearray)):
                      raise RuntimeError("Unexpected binary status response")
                  data = json.loads(raw)
                  if data.get("type") != "status_response" or data.get("status") != "ok":
                      raise RuntimeError(f"Unexpected status response: {data}")
              finally:
                  await ws.close()

          asyncio.run(main())
          PY
      interval: 60s
      timeout: 5s
      retries: 180
      start_period: 120s

  admin-ui:
    image: asterisk-ai-voice-agent-admin-ui:latest
    pull_policy: build
    build:
      context: ./admin_ui
      dockerfile: Dockerfile
    container_name: admin_ui
    network_mode: host
    volumes:
      - ./:/app/project
      - ${DOCKER_SOCK:-/var/run/docker.sock}:/var/run/docker.sock
      # The admin-ui image installs Docker CLI + Compose plugin internally.
      # Optional fallback (host binaries): use `docker compose -f docker-compose.yml -f docker-compose.admin-ui-host-binaries.yml up -d admin-ui`
      - /etc/os-release:/host/etc/os-release:ro
      - ./data:/app/data
    env_file:
      - .env
    environment:
      - PROJECT_ROOT=/app/project
      - UVICORN_HOST=${UVICORN_HOST:-0.0.0.0}
      - UVICORN_PORT=${UVICORN_PORT:-3003}
      - JWT_SECRET=${JWT_SECRET:-}
      # Timezone for consistent timestamp display in Admin UI
      # Configurable via .env (default: America/Phoenix)
      - TZ=${TZ:-America/Phoenix}
    restart: unless-stopped
