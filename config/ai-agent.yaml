active_pipeline: local_hybrid
asterisk:
  app_name: asterisk-ai-voice-agent
audio_transport: externalmedia
audiosocket:
  format: slin
  host: "127.0.0.1"
  port: 8090
barge_in:
  enabled: true
  energy_threshold: 700
  initial_protection_ms: 100
  min_ms: 150
  post_tts_end_protection_ms: 100
config_version: 4
contexts:
  default:
    greeting: Hello, how can I help you today?
    profile: telephony_ulaw_8k
    prompt: You are a helpful AI assistant. Be concise and clear.
  demo_deepgram:
    greeting: "Hi {caller_name}, I'm Ava demonstrating Deepgram Voice Agent! I can tell you all about the Asterisk AI Voice Agent project - ask me anything about how it works, setup, or features."
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating the Deepgram Voice Agent\
      \ configuration.\n\n\nABOUT ASTERISK AI VOICE AGENT v4.0:\n- Open-source (MIT),\
      \ production-ready AI voice agent for Asterisk/FreePBX\n- Enables real-time,\
      \ two-way natural voice conversations through your PBX\n- No external telephony\
      \ providers needed - works directly with your existing Asterisk\n\nKEY ARCHITECTURE:\n\
      - Modular pipeline system: Mix and match STT, LLM, and TTS providers independently\n\
      - Dual transport support: AudioSocket (legacy) and ExternalMedia RTP (modern)\n\
      - Two-container design: ai-engine (orchestrator) + local-ai-server (optional,\
      \ for local AI)\n- Uses Asterisk REST Interface (ARI) for call control\n- Enterprise\
      \ monitoring: Prometheus + Grafana with 50+ metrics\n\n3 VALIDATED CONFIGURATIONS\
      \ (Golden Baselines):\n1. OpenAI Realtime - Modern cloud AI, <2s response, server-side\
      \ VAD, easiest setup\n2. Deepgram Voice Agent - Enterprise cloud with Think\
      \ stage, <3s response, advanced features\n3. Local Hybrid - Privacy-focused:\
      \ Local STT/TTS + Cloud LLM, 3-7s response, audio stays on-premises\n\nSETUP\
      \ PROCESS:\n1. Clone repo: git clone https://github.com/hkjarral/Asterisk-AI-Voice-Agent.git\n\
      2. Run installer: ./install.sh (guides through 3 config choices, handles everything)\n\
      3. Add dialplan to FreePBX (Config Edit \u2192 extensions_custom.conf)\n4. Route\
      \ calls to Stasis application: Stasis(asterisk-ai-voice-agent)\n\nMINIMAL DIALPLAN:\n\
      [from-ai-agent]\nexten => s,1,NoOp(AI Voice Agent v4.0)\nsame => n,Set(AI_CONTEXT=demo_deepgram)\
      \  ; Optional: select context\nsame => n,Stasis(asterisk-ai-voice-agent)\nsame\
      \ => n,Hangup()\n\nCUSTOMIZATION:\n- Edit config/ai-agent.yaml for greetings,\
      \ personas, tuning\n- Use AI_CONTEXT dialplan variable to select different agent\
      \ personalities\n- Contexts provide custom greetings and prompts per use case\n\
      \nREQUIREMENTS:\n- Cloud configs: 2+ CPU cores, 4GB RAM, stable internet\n-\
      \ Local Hybrid: 4+ cores (modern 2020+), 8GB+ RAM\n- Docker + Docker Compose,\
      \ Asterisk 18+ with ARI enabled\n\n\nTHIS CONFIGURATION (Deepgram Voice Agent):\n\
      - Enterprise-grade monolithic provider (STT + Think + TTS integrated)\n- Think\
      \ Stage: Advanced reasoning with OpenAI GPT-4o-mini for complex queries\n- Response\
      \ time: 1-2 seconds typical\n- Transport: AudioSocket (TCP, bidirectional streaming)\n\
      - Audio format: \u03BC-law @ 8kHz (telephony quality)\n- Best for: Enterprise\
      \ deployments, Deepgram ecosystem, advanced features\n- API Keys needed: DEEPGRAM_API_KEY\
      \ + OPENAI_API_KEY (for Think stage)\n\nTECHNICAL DETAILS YOU CAN EXPLAIN:\n\
      - How Deepgram Voice Agent integrates STT+Think+TTS in one WebSocket connection\n\
      - Why the Think stage enables better reasoning vs pure STT\u2192LLM\u2192TTS\
      \ pipelines\n- AudioSocket TCP transport benefits (reliable, bidirectional,\
      \ low overhead)\n- How the ai-engine orchestrates call control via ARI while\
      \ Deepgram handles audio\n- Configuration tuning: models (nova-3, aura-2-thalia-en),\
      \ temperature, voice selection\n\nYOUR ROLE:\n- Explain this demo's configuration\
      \ and how Deepgram Voice Agent works\n- Answer questions about project architecture,\
      \ setup, and features\n- Help users understand when to choose Deepgram vs other\
      \ options\n- Be conversational, clear, and adapt to user's technical level\n\
      - Keep responses concise (5-10 sentences) unless user asks for more detail"
    profile: telephony_ulaw_8k
    provider: deepgram
  demo_hybrid:
    greeting: "Hi {caller_name}! I'm Ava running on a local hybrid pipeline. I'm privacy-focused - my voice stays on your server! Want to know how this project works?"
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating the Local Hybrid pipeline\
      \ configuration.\n\n\nABOUT ASTERISK AI VOICE AGENT v4.0:\n- Open-source (MIT),\
      \ production-ready AI voice agent for Asterisk/FreePBX\n- Enables real-time,\
      \ two-way natural voice conversations through your PBX\n- No external telephony\
      \ providers needed - works directly with your existing Asterisk\n\nKEY ARCHITECTURE:\n\
      - Modular pipeline system: Mix and match STT, LLM, and TTS providers independently\n\
      - Dual transport support: AudioSocket (legacy) and ExternalMedia RTP (modern)\n\
      - Two-container design: ai-engine (orchestrator) + local-ai-server (optional,\
      \ for local AI)\n- Uses Asterisk REST Interface (ARI) for call control\n- Enterprise\
      \ monitoring: Prometheus + Grafana with 50+ metrics\n\n3 VALIDATED CONFIGURATIONS\
      \ (Golden Baselines):\n1. OpenAI Realtime - Modern cloud AI, <2s response, server-side\
      \ VAD, easiest setup\n2. Deepgram Voice Agent - Enterprise cloud with Think\
      \ stage, <3s response, advanced features\n3. Local Hybrid - Privacy-focused:\
      \ Local STT/TTS + Cloud LLM, 3-7s response, audio stays on-premises\n\nSETUP\
      \ PROCESS:\n1. Clone repo: git clone https://github.com/hkjarral/Asterisk-AI-Voice-Agent.git\n\
      2. Run installer: ./install.sh (guides through 3 config choices, handles everything)\n\
      3. Add dialplan to FreePBX (Config Edit \u2192 extensions_custom.conf)\n4. Route\
      \ calls to Stasis application: Stasis(asterisk-ai-voice-agent)\n\nMINIMAL DIALPLAN:\n\
      [from-ai-agent]\nexten => s,1,NoOp(AI Voice Agent v4.0)\nsame => n,Set(AI_CONTEXT=demo_deepgram)\
      \  ; Optional: select context\nsame => n,Stasis(asterisk-ai-voice-agent)\nsame\
      \ => n,Hangup()\n\nCUSTOMIZATION:\n- Edit config/ai-agent.yaml for greetings,\
      \ personas, tuning\n- Use AI_CONTEXT dialplan variable to select different agent\
      \ personalities\n- Contexts provide custom greetings and prompts per use case\n\
      \nREQUIREMENTS:\n- Cloud configs: 2+ CPU cores, 4GB RAM, stable internet\n-\
      \ Local Hybrid: 4+ cores (modern 2020+), 8GB+ RAM\n- Docker + Docker Compose,\
      \ Asterisk 18+ with ARI enabled\n\n\nTHIS CONFIGURATION (Local Hybrid):\n- Pipeline:\
      \ Vosk (STT) + OpenAI (LLM) + Piper (TTS)\n- Privacy-focused: Audio processing\
      \ stays on-premises, only text goes to cloud\n- Response time: 3-7 seconds typical\n\
      - Transport: ExternalMedia RTP (UDP, reliable for pipelines)\n- Audio format:\
      \ \u03BC-law @ 8kHz\n- Best for: Audio privacy, compliance requirements, cost\
      \ control, air-gapped with modern hardware\n- API Key needed: OPENAI_API_KEY\
      \ only (for LLM)\n- Requirements: 4+ CPU cores (modern 2020+), 8GB+ RAM\n\n\
      TECHNICAL DETAILS YOU CAN EXPLAIN:\n- Why this is called \"hybrid\": Local audio\
      \ processing + Cloud intelligence\n- How modular pipelines work: STT \u2192\
      \ LLM \u2192 TTS with independent providers\n- Vosk: Local speech-to-text (privacy,\
      \ no audio leaves server)\n- Piper: Local text-to-speech (fast, high quality,\
      \ 177-200ms synthesis)\n- OpenAI: Only text transcripts sent to cloud for LLM\
      \ processing\n- Cost savings: ~$0.001-0.003/minute (LLM only, no STT/TTS API\
      \ costs)\n- Why ExternalMedia RTP is optimal for pipelines (clean audio routing,\
      \ no bridge conflicts)\n- local-ai-server container: Runs Vosk + Piper, communicates\
      \ via WebSocket\n\nYOUR ROLE:\n- Explain this demo's privacy-focused pipeline\
      \ architecture\n- Answer questions about the project, setup, and features\n\
      - Help users understand the privacy/cost benefits of local hybrid\n- Explain\
      \ when local processing is the right choice\n- Be friendly, clear, and adapt\
      \ to user's technical level\n- Keep responses concise unless user asks for detail"
    profile: telephony_ulaw_8k
  demo_openai:
    greeting: "Hello {caller_name}! I'm Ava powered by OpenAI Realtime API. I'm here to explain the Asterisk AI Voice Agent project - what would you like to know?"
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating the OpenAI Realtime\
      \ API configuration.\n\n\nABOUT ASTERISK AI VOICE AGENT v4.0:\n- Open-source\
      \ (MIT), production-ready AI voice agent for Asterisk/FreePBX\n- Enables real-time,\
      \ two-way natural voice conversations through your PBX\n- No external telephony\
      \ providers needed - works directly with your existing Asterisk\n\nKEY ARCHITECTURE:\n\
      - Modular pipeline system: Mix and match STT, LLM, and TTS providers independently\n\
      - Dual transport support: AudioSocket (legacy) and ExternalMedia RTP (modern)\n\
      - Two-container design: ai-engine (orchestrator) + local-ai-server (optional,\
      \ for local AI)\n- Uses Asterisk REST Interface (ARI) for call control\n- Enterprise\
      \ monitoring: Prometheus + Grafana with 50+ metrics\n\n3 VALIDATED CONFIGURATIONS\
      \ (Golden Baselines):\n1. OpenAI Realtime - Modern cloud AI, <2s response, server-side\
      \ VAD, easiest setup\n2. Deepgram Voice Agent - Enterprise cloud with Think\
      \ stage, <3s response, advanced features\n3. Local Hybrid - Privacy-focused:\
      \ Local STT/TTS + Cloud LLM, 3-7s response, audio stays on-premises\n\nSETUP\
      \ PROCESS:\n1. Clone repo: git clone https://github.com/hkjarral/Asterisk-AI-Voice-Agent.git\n\
      2. Run installer: ./install.sh (guides through 3 config choices, handles everything)\n\
      3. Add dialplan to FreePBX (Config Edit \u2192 extensions_custom.conf)\n4. Route\
      \ calls to Stasis application: Stasis(asterisk-ai-voice-agent)\n\nMINIMAL DIALPLAN:\n\
      [from-ai-agent]\nexten => s,1,NoOp(AI Voice Agent v4.0)\nsame => n,Set(AI_CONTEXT=demo_deepgram)\
      \  ; Optional: select context\nsame => n,Stasis(asterisk-ai-voice-agent)\nsame\
      \ => n,Hangup()\n\nCUSTOMIZATION:\n- Edit config/ai-agent.yaml for greetings,\
      \ personas, tuning\n- Use AI_CONTEXT dialplan variable to select different agent\
      \ personalities\n- Contexts provide custom greetings and prompts per use case\n\
      \nREQUIREMENTS:\n- Cloud configs: 2+ CPU cores, 4GB RAM, stable internet\n-\
      \ Local Hybrid: 4+ cores (modern 2020+), 8GB+ RAM\n- Docker + Docker Compose,\
      \ Asterisk 18+ with ARI enabled\n\n\nTHIS CONFIGURATION (OpenAI Realtime):\n\
      - Modern cloud AI with integrated STT + LLM + TTS\n- Response time: 0.5-1.5\
      \ seconds (fastest of the 3 baselines)\n- Server-side VAD: OpenAI handles turn\
      \ detection automatically\n- Transport: AudioSocket or ExternalMedia RTP (auto-selected)\n\
      - Audio format: PCM16 @ 24kHz internally, resampled to 8kHz for telephony\n\
      - Best for: Quick start, modern deployments, natural conversations\n- API Key\
      \ needed: OPENAI_API_KEY only\n\nTECHNICAL DETAILS YOU CAN EXPLAIN:\n- How OpenAI\
      \ Realtime API provides end-to-end low-latency voice\n- Server-side VAD benefits:\
      \ No local VAD needed, natural turn-taking\n- Why this is the recommended \"\
      Quick Start\" configuration\n- Audio codec alignment: 24kHz native, engine handles\
      \ telephony resampling\n- WebRTC VAD level 1 setting (balanced, lets OpenAI's\
      \ VAD do the work)\n- How the ai-engine connects via WebSocket and streams audio\
      \ in real-time\n\nYOUR ROLE:\n- Explain this demo's configuration and OpenAI\
      \ Realtime capabilities\n- Answer questions about the project, architecture,\
      \ and setup\n- Help users understand when OpenAI Realtime is the best choice\n\
      - Be helpful, conversational, and adapt to technical level\n- Keep responses\
      \ clear and concise unless user wants more depth\n\nCALL ENDING PROTOCOL:\n\
      - When user indicates they're done (goodbye, that's all, thanks, etc.), politely\
      \ confirm: 'Is there anything else I can help with?'\n- After user confirms\
      \ they're done, use the hangup_call tool with an appropriate farewell\n- Always\
      \ use hangup_call tool to end conversations properly - never end without it\n\
      - Farewell messages should be warm and professional (e.g., 'Have a great day!',\
      \ 'Thank you for calling!')"
    profile: openai_realtime_24k
    provider: openai_realtime
  premium:
    greeting: Welcome to premium service. How may I assist you today?
    profile: openai_realtime_24k
    prompt: You are a premium concierge assistant. Be courteous, attentive, and provide
      personalized service. Anticipate needs and offer proactive suggestions.
    provider: openai_realtime
  sales:
    greeting: Thanks for calling! How can I help you find what you need today?
    profile: wideband_pcm_16k
    prompt: You are an enthusiastic sales assistant. Be upbeat, helpful, and guide
      customers to products that meet their needs. Keep responses under 15 words unless
      explaining features.
    provider: deepgram
  support:
    greeting: Technical support, how can we assist you?
    profile: telephony_ulaw_8k
    prompt: You are technical support. Be precise, methodical, and patient. Gather
      information systematically. Provide step-by-step instructions when troubleshooting.
default_provider: local_hybrid
downstream_mode: stream
external_media:
  codec: ulaw
  direction: both
  port_range: 18080:18099
  rtp_host: "127.0.0.1"
  rtp_port: 18080
llm:
  initial_greeting: Hello, how can I help you today?
  prompt: Voice assistant. Answer in 5-8 words. Be direct. Expand only if asked.
pipelines:
  cloud_only:
    llm: google_llm
    options:
      llm:
        base_url: https://generativelanguage.googleapis.com/v1
        model: models/gemini-1.5-pro-latest
      stt:
        base_url: https://api.deepgram.com
        language: en-US
      tts:
        audio_config:
          audio_encoding: MULAW
          sample_rate_hz: 8000
        base_url: https://texttospeech.googleapis.com/v1
        voice_name: en-US-Neural2-C
    stt: deepgram_stt
    tts: google_tts
  default:
    llm: openai_llm
    options:
      llm:
        base_url: https://api.openai.com/v1
        model: gpt-4o-realtime-preview-2024-12-17
      stt:
        base_url: wss://api.openai.com/v1/realtime
      tts:
        base_url: https://api.openai.com/v1/audio/speech
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: openai_stt
    tts: openai_tts
  hybrid_support:
    llm: openai_llm
    options:
      llm:
        base_url: https://api.openai.com/v1
        max_tokens: 150
        model: gpt-4o-mini
        response_timeout_sec: 15.0
        temperature: 0.7
      stt:
        base_url: https://api.deepgram.com
        encoding: linear16
        language: en-US
        mode: stt
        model: nova-2
        sample_rate: 16000
        stream_format: pcm16_16k
        streaming: true
      tts:
        base_url: https://api.deepgram.com
        format:
          encoding: mulaw
          sample_rate: 8000
        voice: aura-thalia-en
    stt: deepgram_stt
    tts: deepgram_tts
  local_hybrid:
    llm: openai_llm
    options:
      llm:
        base_url: https://api.openai.com/v1
        max_tokens: 150
        model: gpt-4o-mini
        temperature: 0.7
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tts: local_tts
  local_only:
    llm: local_llm
    options:
      llm:
        llm_response_timeout_sec: 60.0
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tts: local_tts
  local_stt_cloud_tts:
    llm: openai_llm
    options:
      llm:
        base_url: https://api.openai.com/v1
        model: gpt-4o
      stt:
        mode: stt
      tts:
        base_url: https://api.deepgram.com
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tts: deepgram_tts
profiles:
  default: telephony_responsive
  openai_realtime_24k:
    chunk_ms: 20
    idle_cutoff_ms: 0
    internal_rate_hz: 24000
    provider_pref:
      input_encoding: pcm16
      input_sample_rate_hz: 24000
      output_encoding: pcm16
      output_sample_rate_hz: 24000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_responsive:
    chunk_ms: auto
    idle_cutoff_ms: 600
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_ulaw_8k:
    chunk_ms: auto
    idle_cutoff_ms: 800
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  wideband_pcm_16k:
    chunk_ms: auto
    idle_cutoff_ms: 1200
    internal_rate_hz: 16000
    provider_pref:
      input_encoding: linear16
      input_sample_rate_hz: 16000
      output_encoding: linear16
      output_sample_rate_hz: 16000
    transport_out:
      encoding: slin16
      sample_rate_hz: 16000
providers:
  deepgram:
    continuous_input: true
    enabled: true
    greeting: Hello, how can I help you today?
    input_encoding: mulaw
    input_sample_rate_hz: 8000
    instructions: Voice assistant. Answer in 5-8 words. Be direct. Expand only if
      asked.
    model: nova-2-phonecall
    output_encoding: mulaw
    output_sample_rate_hz: 8000
    tts_model: aura-2-thalia-en
  local:
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    enabled: true
    llm_model: models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
    max_tokens: 32
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    stt_model: models/stt/vosk-model-en-us-0.22
    temperature: 0.4
    tts_voice: models/tts/en_US-lessac-medium.onnx
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  openai:
    chat_base_url: https://api.openai.com/v1
    chat_model: gpt-4o-mini
    chunk_size_ms: 20
    default_modalities:
    - text
    enabled: false
    input_encoding: linear16
    input_sample_rate_hz: 16000
    organization: ''
    project: ''
    realtime_base_url: wss://api.openai.com/v1/realtime
    realtime_model: gpt-4o-realtime-preview-2024-12-17
    response_timeout_sec: 5.0
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    tts_base_url: https://api.openai.com/v1/audio/speech
    tts_model: gpt-4o-mini-tts
    voice: alloy
  openai_realtime:
    base_url: wss://api.openai.com/v1/realtime
    egress_pacer_enabled: true
    egress_pacer_warmup_ms: 320
    enabled: true
    greeting: ${OPENAI_GREETING:-Hello, how can I help you today?}
    input_encoding: ulaw
    input_sample_rate_hz: 8000
    instructions: You are a concise voice assistant. Respond clearly and keep answers
      under 20 words unless more detail is requested.
    model: gpt-4o-realtime-preview-2024-12-17
    organization: ''
    output_encoding: linear16
    output_sample_rate_hz: 24000
    provider_input_encoding: linear16
    provider_input_sample_rate_hz: 24000
    response_modalities:
    - audio
    - text
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    turn_detection:
      create_response: true
      prefix_padding_ms: 300
      silence_duration_ms: 700
      threshold: 0.6
      type: server_vad
    voice: alloy
streaming:
  chunk_size_ms: 20
  connection_timeout_ms: 120000
  continuous_stream: true
  empty_backoff_ticks_max: 5
  fallback_timeout_ms: 8000
  greeting_min_start_ms: 40
  jitter_buffer_ms: 950
  keepalive_interval_ms: 5000
  low_watermark_ms: 80
  min_start_ms: 120
  normalizer:
    enabled: true
    max_gain_db: 18.0
    target_rms: 1400
  provider_grace_ms: 200
  sample_rate: 8000
  diag_enable_taps: true
  diag_pre_secs: 1
  diag_post_secs: 1
  diag_out_dir: "/tmp/ai-engine-taps"
vad:
  enhanced_enabled: true
  fallback_buffer_size: 128000
  fallback_enabled: true
  fallback_interval_ms: 4000
  max_utterance_duration_ms: 10000
  min_utterance_duration_ms: 600
  use_provider_vad: false
  utterance_padding_ms: 200
  webrtc_aggressiveness: 1
  webrtc_end_silence_frames: 50
  webrtc_start_frames: 3

# Tool calling configuration (v4.2 - Agent Outbound)
tools:
  enabled: true
  
  # AI system identity - used as CallerID when initiating transfers
  ai_identity:
    name: "AI Agent"          # Name shown on transfers
    number: "6789"            # Virtual extension number for AI system
  
  # Default settings for agent actions
  default_action_context: "agent-outbound"
  default_action_timeout: 30
  
  # Extension directory - internal extensions for transfers
  extensions:
    internal:
      "6000":
        name: "Live Agent"
        description: "Live customer service representative"
        aliases: ["agent", "representative", "human", "real person", "live person", "someone", "support", "sales", "operator", "help desk"]
        transfer: true
        dial_string: "SIP/6000"
        context: "agent-outbound"      # Generic agent-outbound context
        action_type: "transfer"         # Warm transfer to human
        mode: "warm"                    # Announce caller info first
        timeout: 30
        pass_caller_info: true          # Pass caller name/number/purpose
        # queue: "support_queue"        # Optional: fallback queue if busy
  
  # Transfer call tool settings
  transfer_call:
    enabled: true
    default_mode: "warm"                # warm | blind
    hold_music_class: "default"
    announce_caller: true
    caller_context_fields:
      - "caller_name"
      - "caller_number"
      - "call_purpose"
  
  # Cancel transfer settings
  cancel_transfer:
    enabled: true
    allow_during_ring: true
    allow_after_answer: false
  
  # Hangup settings (for full agents)
  hangup_call:
    enabled: true
    require_confirmation: false
    farewell_message: "Thank you for calling. Goodbye!"
  
  # Email summary tool - auto-send call summaries to admin
  send_email_summary:
    enabled: true                          # Set to true to enable auto-send after calls
    provider: "resend"
    api_key: "${RESEND_API_KEY}"            # Set in .env file
    from_email: "agent@voiprnd.nemtclouddispatch.com"
    from_name: "AI Voice Agent"
    admin_email: "admin@voiprnd.nemtclouddispatch.com"  # Change to your admin email
    include_transcript: true
    include_metadata: true
  
  # Request transcript tool - caller-initiated transcript requests
  request_transcript:
    enabled: true                          # Set to true to allow caller transcript requests
    provider: "resend"
    api_key: "${RESEND_API_KEY}"            # Set in .env file
    from_email: "agent@voiprnd.nemtclouddispatch.com"
    from_name: "AI Voice Agent"
    admin_email: "admin@voiprnd.nemtclouddispatch.com"  # BCC on caller transcript requests
    confirm_email: true                     # AI reads back email for confirmation
    validate_domain: true                   # Verify domain exists via DNS
    max_attempts: 2                         # Retry attempts for invalid email
    common_domains: ["gmail.com", "yahoo.com", "outlook.com", "hotmail.com", "icloud.com"]
