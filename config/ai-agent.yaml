# config/ai-agent.yaml
# Default provider to use if not specified in the dialplan
default_provider: "deepgram"

# Pipeline definitions specify which STT/LLM/TTS components power each call.
# Pipelines can mix local and cloud services; the orchestrator selects components
# based on `active_pipeline`. Per-component options override defaults (base URLs,
# models, voices, codec formats, etc.).
pipelines:
  # Hosted OpenAI realtime stack across STT/LLM/TTS. Requires OPENAI_API_KEY for bearer auth.
  default:
    stt: openai_stt
    llm: openai_llm
    tts: openai_tts
    options:
      stt:
        base_url: "wss://api.openai.com/v1/realtime"  # Bearer auth with OPENAI_API_KEY
      llm:
        base_url: "https://api.openai.com/v1"
        model: "gpt-4o-realtime-preview-2024-12-17"
      tts:
        base_url: "https://api.openai.com/v1/audio/speech"
        format:
          encoding: "mulaw"  # Match telephony trunk for immediate playback
          sample_rate: 8000
  # Fully local inference pipeline; runs entirely inside local_ai_server.
  local_only:
    stt: local_stt
    llm: local_llm
    tts: local_tts
    options:
      stt:
        mode: "stt"  # Restrict local provider to speech recognition only
        chunk_ms: 160  # Lower chunk size for faster cadence while streaming
        streaming: true  # Enable continuous streaming via send_audio/iter_results
        stream_format: "pcm16_16k"
      llm:
        llm_response_timeout_sec: 60.0  # Allow slower local LLM to respond within this window
      tts:
        format:
          encoding: "mulaw"  # Matches telephony trunks without resampling
          sample_rate: 8000
  # Hybrid support: local STT/TTS with OpenAI LLM. Targets milestone 7 validation.
  hybrid_support:
    stt: local_stt
    llm: openai_llm
    tts: deepgram_tts
    options:
      stt:
        mode: "stt"
        chunk_ms: 320
        streaming: true
        stream_format: "pcm16_16k"
      llm:
        base_url: "https://api.openai.com/v1"
        model: "gpt-4o-mini"
        temperature: 0.6
        max_tokens: 120
        response_timeout_sec: 15.0
      tts:
        base_url: "https://api.deepgram.com"
        voice: "aura-asteria-en"
        format:
          encoding: "mulaw"
          sample_rate: 8000
  # Hybrid pipeline: local STT + OpenAI LLM + Deepgram Aura TTS.
  local_stt_cloud_tts:
    stt: local_stt
    llm: openai_llm
    tts: deepgram_tts
    options:
      stt:
        mode: "stt"  # Local provider ingests AudioSocket frames for recognition
      llm:
        base_url: "https://api.openai.com/v1"
        model: "gpt-4o"  # Uses OPENAI_API_KEY via bearer auth
      tts:
        base_url: "https://api.deepgram.com"  # Requires DEEPGRAM_API_KEY
        format:
          encoding: "mulaw"  # Deepgram Aura emits 8 kHz μ-law ready for playback
          sample_rate: 8000
  # Cloud vendor mix: Deepgram STT + Google Gemini LLM + Google TTS.
  cloud_only:
    stt: deepgram_stt
    llm: google_llm
    tts: google_tts
    options:
      stt:
        base_url: "https://api.deepgram.com"  # Authenticates with DEEPGRAM_API_KEY
        language: "en-US"  # See Deepgram docs for supported locales
      llm:
        base_url: "https://generativelanguage.googleapis.com/v1"
        model: "models/gemini-1.5-pro-latest"  # Requires Google API credentials
      tts:
        base_url: "https://texttospeech.googleapis.com/v1"
        voice_name: "en-US-Neural2-C"
        audio_config:
          audio_encoding: "MULAW"  # Telephony μ-law output for direct playback
          sample_rate_hz: 8000

active_pipeline: "default"

# Audio transport controls (AudioSocket-first, ExternalMedia as fallback)
#
# Tuning notes:
# - `audio_transport`: prefer `audiosocket` for full-duplex PCM over TCP; use `externalmedia` for RTP.
# - `downstream_mode`:
#     - `stream`: streams agent audio in 20 ms frames in real time.
#     - `file`: plays μ-law files via bridge; more tolerant but higher latency.
audio_transport: "audiosocket"
downstream_mode: "stream"

# AudioSocket listener configuration (when audio_transport=audiosocket)
audiosocket:
  host: "0.0.0.0"          # Bind address (use 127.0.0.1 when engine and Asterisk share a host)
  port: 8090               # TCP port for AudioSocket connections
  format: "ulaw"             # Wire format: μ-law @ 8kHz for AudioSocket (native telephony encoding)

# Barge-in configuration
barge_in:
  enabled: true
  initial_protection_ms: 100   # Short initial guard so caller audio reaches provider quickly
                               # Range: 200–600 ms (higher if trunks echo or agent intros are long)
  min_ms: 250                  # Sustained speech required to trigger barge-in (de-bounce)
                               # Range: 250–600 ms (lower = more sensitive barge-in)
  energy_threshold: 1100       # RMS threshold for speech detection (telephony baseline)
                               # Range: 1000–3000 (raise on noisy lines)
                               # Range: 500–1500 ms
  post_tts_end_protection_ms: 100  # Minimal post-TTS guard to avoid echo without starving input
                                   # Range: 250–500 ms (lower to allow faster user pickup)

# Streaming configuration (for downstream_mode=stream)
streaming:
  sample_rate: 8000           # Output sample rate for streaming audio (telephony μ-law baseline)
  jitter_buffer_ms: 950      # Larger buffer to reduce underflows during provider silence
  keepalive_interval_ms: 5000 # Keepalive interval for streaming connections
  connection_timeout_ms: 120000 # Extended to tolerate long provider pauses in continuous mode
  fallback_timeout_ms: 8000   # No audio sent for this long → fall back to file playback
  chunk_size_ms: 20          # Wire pacing for AudioSocket: 20 ms frames (320 bytes PCM16 @ 8kHz)
  min_start_ms: 700          # Longer warm-up to stabilize buffer before first frame
  low_watermark_ms: 350      # Pause when depth < ~2/3 of min_start; 350ms keeps some cushion.
  provider_grace_ms: 60      # Short tail grace so cleanup finishes quickly without dragging cadence.
  logging_level: "debug"      # Optional override for streaming logger verbosity
  egress_swap_mode: "auto"   # Auto: follow inbound slin16 swap probe for PCM16 byte order
  egress_force_mulaw: false
  attack_ms: 0               # DISABLED: Attack envelope was creating initial silence
  # Maintain a single continuous stream across provider segments (prevents segment loss)
  continuous_stream: true
  # Adaptive low-buffer backoff (reduce filler churn during provider gaps)
  empty_backoff_ticks_max: 5
  # Audio normalizer (make-up gain before μ-law encode)
  normalizer:
    enabled: true
    target_rms: 1400
    max_gain_db: 18.0  # Increased to allow ~6x boost for quiet provider audio
  # Diagnostics (feature-flagged): capture short PCM taps pre/post compand for next call
  diag_enable_taps: true
  diag_pre_secs: 1
  diag_post_secs: 1
  diag_out_dir: "/tmp/ai-engine-taps"

# Asterisk connection settings (will be overridden by environment variables)
asterisk:
  host: "your_asterisk_host"
  username: "your_ari_username"
  app_name: "asterisk-ai-voice-agent"

# External Media configuration for RTP-based audio capture
external_media:
  rtp_host: "0.0.0.0"      # target IP for ExternalMedia (localhost for host networking)
  rtp_port: 18080            # fixed port for simplicity
  port_range: "18080:18099"  # optional range for dynamic allocation (start:end); leave empty to reuse rtp_port
  codec: "ulaw"              # ulaw (8k) or slin16 (8k)
  direction: "both"          # sendrecv | sendonly | recvonly
  jitter_buffer_ms: 20       # target frame size


# Global LLM settings (will be overridden by environment variables)
llm:
  initial_greeting: "Hello, how can I help you today?"
  prompt: "You are a concise and helpful voice assistant. Keep replies under 20 words unless asked for detail."
  model: "gpt-4o"
  api_key: "${OPENAI_API_KEY}" # Example of using env var substitution


# VAD Configuration - Optimized for 4+ second utterances
vad:
  use_provider_vad: false      # Rely on local WebRTC VAD for turn detection
  enhanced_enabled: false      # Disable local enhanced VAD to avoid double-gating
  # WebRTC VAD settings
  webrtc_aggressiveness: 0   # Least aggressive (0-3) for telephony audio
  webrtc_start_frames: 3     # Consecutive frames to start recording
  webrtc_end_silence_frames: 50  # Silence frames to end recording (1000ms)
  
  # Utterance settings - optimized for 4+ second utterances
  min_utterance_duration_ms: 600  # Minimum ~600 ms to allow faster turn-taking
  max_utterance_duration_ms: 10000 # Maximum 10 seconds
  utterance_padding_ms: 200        # Padding before/after speech
  
  # Fallback settings
  fallback_enabled: true           # Enable fallback when VAD fails
  fallback_interval_ms: 4000       # Send audio every 4 seconds as fallback
  fallback_buffer_size: 128000     # 4 seconds of 16kHz audio (128,000 bytes)

# Provider-specific configurations
providers:
  local:
    enabled: false
    ws_url: "${LOCAL_WS_URL:-ws://127.0.0.1:8765}"
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=5.0}
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    stt_model: "models/stt/vosk-model-small-en-us-0.15"   # legacy placeholder (Local AI server now loads via env vars)
    llm_model: "models/llm/TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf"
    tts_voice: "models/tts/en_US-lessac-medium.onnx"
    temperature: 0.4
    max_tokens: 64
  deepgram:
    enabled: true
    api_key: "${DEEPGRAM_API_KEY}"
    model: "nova-2-phonecall"
    tts_model: "aura-2-thalia-en"
    greeting: "Hello, how can I help you today?"
    instructions: "You are a concise voice assistant. Respond in under 20 words and answer immediately."
    input_encoding: "mulaw"
    input_sample_rate_hz: 8000
    continuous_input: true        # Stream audio continuously for best responsiveness
                                  # TIP: Keep this true for real-time conversations
    allow_output_autodetect: false
    output_encoding: "mulaw"
    output_sample_rate_hz: 8000
  openai:
    enabled: false
    api_key: "${OPENAI_API_KEY}"
    organization: ""
    project: ""
    realtime_base_url: "wss://api.openai.com/v1/realtime"
    chat_base_url: "https://api.openai.com/v1"
    tts_base_url: "https://api.openai.com/v1/audio/speech"
    realtime_model: "gpt-4o-realtime-preview-2024-12-17"
    chat_model: "gpt-4o-mini"
    tts_model: "gpt-4o-mini-tts"
    voice: "alloy"
    default_modalities:
      - "text"
    input_encoding: "linear16"
    input_sample_rate_hz: 16000
    target_encoding: "mulaw"
    target_sample_rate_hz: 8000
    chunk_size_ms: 20
    response_timeout_sec: 5.0
  openai_realtime:
    enabled: false
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4o-realtime-preview-2024-12-17"
    voice: "alloy"
    base_url: "wss://api.openai.com/v1/realtime"
    instructions: "You are a concise voice assistant. Respond clearly and keep answers under 20 words unless more detail is requested."
    organization: ""
    input_encoding: "ulaw"
    input_sample_rate_hz: 8000
    provider_input_encoding: "linear16"
    provider_input_sample_rate_hz: 24000
    output_encoding: "linear16"
    output_sample_rate_hz: 24000
    target_encoding: "mulaw"
    target_sample_rate_hz: 8000
    response_modalities:
      - "audio"
      - "text"
    # Explicit greeting said immediately on connect via response.create
    greeting: "${OPENAI_GREETING:-Hello, how can I help you today?}"
    # Optional: enable server-side VAD turn detection to improve turn handling
    turn_detection:
      type: "server_vad"
      silence_duration_ms: 200
      threshold: 0.5
      prefix_padding_ms: 200
    # Egress pacer for telephony cadence (20ms frames) with warm-up
    egress_pacer_enabled: true
    egress_pacer_warmup_ms: 320
