# config/ai-agent.yaml
# Default provider to use if not specified in the dialplan
default_provider: "openai_realtime"

# Pipeline definitions specify which STT/LLM/TTS components power each call.
# Pipelines can mix local and cloud services; the orchestrator selects components
# based on `active_pipeline`. Per-component options override defaults (base URLs,
# models, voices, codec formats, etc.).
pipelines:
  # Hosted OpenAI realtime stack across STT/LLM/TTS. Requires OPENAI_API_KEY for bearer auth.
  default:
    stt: openai_stt
    llm: openai_llm
    tts: openai_tts
    options:
      stt:
        base_url: "wss://api.openai.com/v1/realtime"  # Bearer auth with OPENAI_API_KEY
      llm:
        base_url: "https://api.openai.com/v1"
        model: "gpt-4o-realtime-preview-2024-12-17"
      tts:
        base_url: "https://api.openai.com/v1/audio/speech"
        format:
          encoding: "linear16"  # Provider emits PCM16 frames used by streaming mode
          sample_rate: 24000    # Engine resamples to downstream target sample rate as needed
  # Fully local inference pipeline; runs entirely inside local_ai_server.
  local_only:
    stt: local_stt
    llm: local_llm
    tts: local_tts
    options:
      stt:
        mode: "stt"  # Restrict local provider to speech recognition only
        chunk_ms: 160  # Lower chunk size for faster cadence while streaming
        streaming: true  # Enable continuous streaming via send_audio/iter_results
        stream_format: "pcm16_16k"
      llm:
        llm_response_timeout_sec: 60.0  # Allow slower local LLM to respond within this window
      tts:
        format:
          encoding: "mulaw"  # Matches telephony trunks without resampling
          sample_rate: 8000
  # Hybrid support: local STT/TTS with OpenAI LLM. Targets milestone 7 validation.
  hybrid_support:
    stt: local_stt
    llm: openai_llm
    tts: deepgram_tts
    options:
      stt:
        mode: "stt"
        chunk_ms: 320
        streaming: true
        stream_format: "pcm16_16k"
      llm:
        base_url: "https://api.openai.com/v1"
        model: "gpt-4o-mini"
        temperature: 0.6
        max_tokens: 120
        response_timeout_sec: 15.0
      tts:
        base_url: "https://api.deepgram.com"
        voice: "aura-asteria-en"
        format:
          encoding: "mulaw"
          sample_rate: 8000
  # Hybrid pipeline: local STT + OpenAI LLM + Deepgram Aura TTS.
  local_stt_cloud_tts:
    stt: local_stt
    llm: openai_llm
    tts: deepgram_tts
    options:
      stt:
        mode: "stt"  # Local provider ingests AudioSocket frames for recognition
      llm:
        base_url: "https://api.openai.com/v1"
        model: "gpt-4o"  # Uses OPENAI_API_KEY via bearer auth
      tts:
        base_url: "https://api.deepgram.com"  # Requires DEEPGRAM_API_KEY
        format:
          encoding: "mulaw"  # Deepgram Aura emits 8 kHz μ-law ready for playback
          sample_rate: 8000
  # Cloud vendor mix: Deepgram STT + Google Gemini LLM + Google TTS.
  cloud_only:
    stt: deepgram_stt
    llm: google_llm
    tts: google_tts
    options:
      stt:
        base_url: "https://api.deepgram.com"  # Authenticates with DEEPGRAM_API_KEY
        language: "en-US"  # See Deepgram docs for supported locales
      llm:
        base_url: "https://generativelanguage.googleapis.com/v1"
        model: "models/gemini-1.5-pro-latest"  # Requires Google API credentials
      tts:
        base_url: "https://texttospeech.googleapis.com/v1"
        voice_name: "en-US-Neural2-C"
        audio_config:
          audio_encoding: "MULAW"  # Telephony μ-law output for direct playback
          sample_rate_hz: 8000

active_pipeline: "default"

# Audio transport controls (AudioSocket-first, ExternalMedia as fallback)
#
# Tuning notes:
# - `audio_transport`: prefer `audiosocket` for full-duplex PCM over TCP; use `externalmedia` for RTP.
# - `downstream_mode`:
#     - `stream`: streams agent audio in 20 ms frames in real time.
#     - `file`: plays μ-law files via bridge; more tolerant but higher latency.
audio_transport: "audiosocket"
downstream_mode: "stream"

# AudioSocket listener configuration (when audio_transport=audiosocket)
audiosocket:
  host: "0.0.0.0"          # Bind address (use 127.0.0.1 when engine and Asterisk share a host)
  port: 8090               # TCP port for AudioSocket connections
  format: "ulaw"           # Wire format from Asterisk: `ulaw` (160B/20ms) or `slin16` (320B/20ms) at 8 kHz
                           # TIP: Keep provider input sample rate aligned to 8 kHz when using telephony trunks.

# Barge-in configuration
barge_in:
  enabled: true
  initial_protection_ms: 1500   # Enough guard to avoid self-echo without stalling caller audio
  min_ms: 500                   # Half-second of confident speech triggers barge-in
  energy_threshold: 1700        # Align with adaptive VAD baseline for quiet voices
  cooldown_ms: 1200             # Prevent rapid re-triggers while staying responsive
  post_tts_end_protection_ms: 200  # Short settle time after playback ends
  greeting_protection_ms: 1400  # Extra protection only for the very first greeting

# Streaming configuration (for downstream_mode=stream)
streaming:
  sample_rate: 8000          # Output sample rate for streaming audio
  jitter_buffer_ms: 160      # Eight 20ms frames for steadier playback
  keepalive_interval_ms: 5000 # Keepalive interval for streaming connections
  connection_timeout_ms: 10000 # Connection timeout for streaming
  fallback_timeout_ms: 60000  # Allow extended greeting debugging before falling back
  chunk_size_ms: 20          # Frame size; 20 ms recommended for telephony cadence
  min_start_ms: 100          # Start once we have five buffered frames
  greeting_min_start_ms: 40  # Extra-fast start specifically for greeting
  low_watermark_ms: 60       # Pause only when buffer drains below three frames
  provider_grace_ms: 1400    # Longer grace to avoid tail clipping on stop
  logging_level: "debug"     # Optional override for streaming logger verbosity

# Logging configuration for this run
logging:
  level: "debug"

# Asterisk connection settings (will be overridden by environment variables)
asterisk:
  host: "your_asterisk_host"
  username: "your_ari_username"
  password: "your_ari_password"
  app_name: "asterisk-ai-voice-agent"

# External Media configuration for RTP-based audio capture
external_media:
  rtp_host: "0.0.0.0"      # target IP for ExternalMedia (localhost for host networking)
  rtp_port: 18080            # fixed port for simplicity
  codec: "ulaw"              # ulaw (8k) or slin16 (8k)
  direction: "both"          # sendrecv | sendonly | recvonly
  jitter_buffer_ms: 20       # target frame size


# Global LLM settings (will be overridden by environment variables)
llm:
  initial_greeting: "Hello, how can I help you today?"
  prompt: "You are a concise and helpful voice assistant. Keep replies under 20 words unless asked for detail."
  api_key: "${OPENAI_API_KEY}" # Example of using env var substitution


# VAD Configuration - Optimized for 4+ second utterances
vad:
  use_provider_vad: true      # Defer speech detection to provider (OpenAI server-side VAD)
  enhanced_enabled: false     # Disable local enhanced VAD; leave settings for fallback tuning
  # WebRTC VAD settings - optimized for real-time conversation
  webrtc_aggressiveness: 1   # Slightly more aggressive (0-3) for telephony audio
  webrtc_start_frames: 2     # Consecutive frames to start recording (40ms)
  webrtc_end_silence_frames: 15  # Consecutive silence frames to end (300ms) - telephony standard
  energy_threshold: 1500     # Base RMS energy threshold for speech (lowered for sensitivity)
  confidence_threshold: 0.6  # Minimum enhanced VAD confidence to count as speech
  adaptive_threshold_enabled: true   # Enable adaptive thresholds for better noise handling
  noise_adaptation_rate: 0.2
  # Utterance settings - optimized for real-time conversation
  min_utterance_duration_ms: 600   # Minimum 600ms keeps snappy handoffs
  max_utterance_duration_ms: 8000  # Maximum 8 seconds (reduced from 10000ms)
  utterance_padding_ms: 100        # Padding before/after speech (reduced for responsiveness)
  # Fallback settings - ensure audio flows when VAD fails
  fallback_enabled: true           # Enable fallback when VAD fails to detect speech
  fallback_interval_ms: 1000       # Periodic fallback after 1s silence (prevents starvation)
  fallback_buffer_size: 128000     # 4 seconds of 16kHz audio (128,000 bytes)

# Provider-specific configurations
providers:
  local:
    enabled: false
    ws_url: "${LOCAL_WS_URL:-ws://127.0.0.1:8765}"
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=5.0}
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    stt_model: "models/stt/vosk-model-small-en-us-0.15"   # legacy placeholder (Local AI server now loads via env vars)
    llm_model: "models/llm/TinyLlama-1.1B-Chat-v1.0.Q4_K_M.gguf"
    tts_voice: "models/tts/en_US-lessac-medium.onnx"
    temperature: 0.4
    max_tokens: 64
  deepgram:
    enabled: true
    api_key: "${DEEPGRAM_API_KEY}"
    model: "nova-2-general"
    tts_model: "aura-asteria-en"
    greeting: "${DEEPGRAM_GREETING:-Hello, how can I help you today?}"
    instructions: "${DEEPGRAM_INSTRUCTIONS:-You are a concise voice assistant. Respond in under 20 words and answer immediately.}"
    input_encoding: "linear16"   # Provider audio encoding for inbound caller audio
    input_sample_rate_hz: 8000    # Align to trunk sample rate (8 kHz for telephony)
    continuous_input: true        # Stream audio continuously for best responsiveness
                                  # TIP: Keep this true for real-time conversations
  openai:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
    organization: ""
    project: ""
    realtime_base_url: "wss://api.openai.com/v1/realtime"
    chat_base_url: "https://api.openai.com/v1"
    tts_base_url: "https://api.openai.com/v1/audio/speech"
    realtime_model: "gpt-4o-realtime-preview-2024-12-17"
    chat_model: "gpt-4o-mini"
    tts_model: "gpt-4o-mini-tts"
    voice: "alloy"
    default_modalities:
      - "text"
    input_encoding: "linear16"
    input_sample_rate_hz: 16000
    target_encoding: "mulaw"
    target_sample_rate_hz: 8000
    chunk_size_ms: 20
    response_timeout_sec: 5.0
  openai_realtime:
    enabled: true
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4o-realtime-preview-2024-12-17"
    voice: "alloy"
    base_url: "wss://api.openai.com/v1/realtime"
    instructions: "You are a concise voice assistant. Respond clearly and keep answers under 20 words unless more detail is requested."
    organization: ""
    input_encoding: "ulaw"             # Match AudioSocket wire format (μ-law 8 kHz)
    input_sample_rate_hz: 8000         # Engine will resample to provider_input_sample_rate_hz as needed
    provider_input_sample_rate_hz: 24000
    output_encoding: "linear16"       # Provider emits PCM16 frames
    output_sample_rate_hz: 24000
    target_encoding: "ulaw"  # Explicit for file playback; update if enabling streaming PCM
    target_sample_rate_hz: 8000
    response_modalities:
      - "audio"
      - "text"
    # Explicit greeting said immediately on connect via response.create
    greeting: "${OPENAI_GREETING:-Hello, how can I help you today?}"
    # Optional: enable server-side VAD turn detection to improve turn handling
    turn_detection:
      type: "server_vad"
      silence_duration_ms: 650   # Avoid early stream cutoffs
      threshold: 0.6             # Slightly less sensitive
      prefix_padding_ms: 150     # More context before speech
