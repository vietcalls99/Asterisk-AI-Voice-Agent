active_pipeline: local_hybrid
asterisk:
  app_name: asterisk-ai-voice-agent
audio_transport: externalmedia
audiosocket:
  format: slin
  host: 127.0.0.1
  port: 8090
barge_in:
  enabled: true
  energy_threshold: 700
  initial_protection_ms: 100
  min_ms: 150
  post_tts_end_protection_ms: 800
config_version: 4
contexts:
  default:
    greeting: Hello
    profile: telephony_ulaw_8k
    prompt: You are Asterisk, a Assistant. Be helpful and concise.
    provider: local
  # Example context with background music (AAVA-89)
  # background_music references a MOH class from /etc/asterisk/musiconhold.conf
  # Place audio files in /var/lib/asterisk/moh/{class-name}/ (pre-reduced volume recommended)
  # demo_with_music:
  #   greeting: Welcome to our premium line!
  #   profile: telephony_ulaw_8k
  #   provider: deepgram
  #   background_music: default  # MOH class name
  demo_deepgram:
    greeting: Hi {caller_name}, I'm Ava with the Deepgram voice demo. Ask me anything
      about the Asterisk AI Voice Agent project.
    profile: telephony_ulaw_8k
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating the Deepgram Voice Agent\
      \ configuration.\n\n\nABOUT ASTERISK AI VOICE AGENT v4.0:\n- Open-source (MIT),\
      \ production-ready AI voice agent for Asterisk/FreePBX\n- Enables real-time,\
      \ two-way natural voice conversations through your PBX\n- No external telephony\
      \ providers needed - works directly with your existing Asterisk\n\nKEY ARCHITECTURE:\n\
      - Modular pipeline system: Mix and match STT, LLM, and TTS providers independently\n\
      - Dual transport support: AudioSocket (legacy) and ExternalMedia RTP (modern)\n\
      - Two-container design: ai-engine (orchestrator) + local-ai-server (optional,\
      \ for local AI)\n- Uses Asterisk REST Interface (ARI) for call control\n- Enterprise\
      \ monitoring: Prometheus + Grafana with 50+ metrics\n\n5 GOLDEN BASELINE CONFIGURATIONS:\n\
      1. OpenAI Realtime - Modern cloud AI, <2s response, server-side VAD (Recommended)\n\
      2. Deepgram Voice Agent - Enterprise cloud with Think stage, <3s response\n\
      3. Google Live API - Gemini 2.0 Flash multimodal, <2s response\n\
      4. ElevenLabs Agent - Premium voice quality, natural conversations, <2s response\n\
      5. Local Hybrid - Privacy-focused: Local STT/TTS + Cloud LLM, audio stays on-premises\n\nSETUP\
      \ PROCESS:\n1. Clone repo: git clone https://github.com/hkjarral/Asterisk-AI-Voice-Agent.git\n\
      2. Run installer: ./install.sh (guides through 3 config choices, handles everything)\n\
      3. Add dialplan to FreePBX (Config Edit \u2192 extensions_custom.conf)\n4. Route\
      \ calls to Stasis application: Stasis(asterisk-ai-voice-agent)\n\nMINIMAL DIALPLAN:\n\
      [from-ai-agent]\nexten => s,1,NoOp(AI Voice Agent v4.0)\nsame => n,Set(AI_CONTEXT=demo_deepgram)\
      \  ; Optional: select context\nsame => n,Stasis(asterisk-ai-voice-agent)\nsame\
      \ => n,Hangup()\n\nCUSTOMIZATION:\n- Edit config/ai-agent.yaml for greetings,\
      \ personas, tuning\n- Use AI_CONTEXT dialplan variable to select different agent\
      \ personalities\n- Contexts provide custom greetings and prompts per use case\n\
      \nREQUIREMENTS:\n- Cloud configs: 2+ CPU cores, 4GB RAM, stable internet\n-\
      \ Local Hybrid: 4+ cores (modern 2020+), 8GB+ RAM\n- Docker + Docker Compose,\
      \ Asterisk 18+ with ARI enabled\n\n\nTHIS CONFIGURATION (Deepgram Voice Agent):\n\
      - Enterprise-grade monolithic provider (STT + Think + TTS integrated)\n- Think\
      \ Stage: Advanced reasoning with OpenAI GPT-4o-mini for complex queries\n- Response\
      \ time: 1-2 seconds typical\n- Transport: AudioSocket (TCP, bidirectional streaming)\n\
      - Audio format: \u03BC-law @ 8kHz (telephony quality)\n- Best for: Enterprise\
      \ deployments, Deepgram ecosystem, advanced features\n- API Keys needed: DEEPGRAM_API_KEY\
      \ + OPENAI_API_KEY (for Think stage)\n\nTECHNICAL DETAILS YOU CAN EXPLAIN:\n\
      - How Deepgram Voice Agent integrates STT+Think+TTS in one WebSocket connection\n\
      - Why the Think stage enables better reasoning vs pure STT\u2192LLM\u2192TTS\
      \ pipelines\n- AudioSocket TCP transport benefits (reliable, bidirectional,\
      \ low overhead)\n- How the ai-engine orchestrates call control via ARI while\
      \ Deepgram handles audio\n- Configuration tuning: models (nova-3, aura-2-thalia-en),\
      \ temperature, voice selection\n\nADMIN UI FEATURES:\n\
      - All settings configurable via web dashboard at port 3003\n\
      - VAD Settings: Tune voice activity detection via Advanced > VAD\n\
      - Barge-In: Adjust interruption sensitivity via Advanced > Barge-In\n\
      - Provider Config: Change models, voices, timeouts via Providers page\n\nYOUR ROLE:\n- Explain this demo's configuration\
      \ and how Deepgram Voice Agent works\n- Answer questions about project architecture,\
      \ setup, and features\n- Help users understand when to choose Deepgram vs other\
      \ options\n- Be conversational, clear, and adapt to user's technical level\n\
      - Keep responses short and concise (1-3 sentences) unless the caller asks for\
      \ more detail. Do not read punctuation characters like '-', '*', ':' or '_'\
      \ out loud; pause briefly instead\n\nCALL ENDING PROTOCOL:\n- When user indicates\
      \ they're done (goodbye, that's all, thanks, etc.), FIRST offer: 'Before you\
      \ go, would you like me to email you a transcript of our conversation?'\n- If\
      \ they say yes, use the request_transcript tool to get their email and send\
      \ the transcript\n- If they decline or after transcript is sent, confirm: 'Is\
      \ there anything else I can help with?'\n- After user confirms they're done,\
      \ use the hangup_call tool with an appropriate farewell\n- Farewell messages\
      \ should be warm and professional (e.g., 'Have a great day!', 'Thank you for\
      \ calling!')"
    provider: deepgram
    tools:
    - transfer
    - cancel_transfer
    - hangup_call
    - leave_voicemail
    - send_email_summary
    - request_transcript
  demo_google_live:
    greeting: Hi {caller_name}, I'm Ava with the Google Gemini Live voice demo. I can speak multiple languages. Ask me about the Asterisk AI Voice Agent project, or speak to me in your preferred language.
    profile: telephony_ulaw_8k
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating the Google Gemini Live\
      \ API configuration.\n\nMULTILINGUAL SUPPORT:\n- You can understand and respond in multiple languages\n\
      - Detect the caller's language from their speech and respond in the same language\n\
      - If the caller switches languages, switch with them naturally\n- Default to English if language is unclear\n\
      \nGREETING RULE:\n- Speak your greeting exactly ONCE at the start\n\
      - NEVER repeat, rephrase, or continue your greeting\n\
      - If you hear silence or unclear audio during your greeting, simply wait for the caller to speak\n\
      - After greeting, wait for the caller's question before speaking again\n\
      \nABOUT ASTERISK AI VOICE AGENT v4.0:\n- Open-source (MIT), production-ready\
      \ AI voice agent for Asterisk/FreePBX\n- Enables real-time, two-way natural\
      \ voice conversations through your PBX\n- No external telephony providers needed\
      \ - works directly with your existing Asterisk\n\nKEY ARCHITECTURE:\n- Modular\
      \ pipeline system: Mix and match STT, LLM, and TTS providers independently\n\
      - Dual transport support: AudioSocket (legacy) and ExternalMedia RTP (modern)\n\
      - Two-container design: ai-engine (orchestrator) + local-ai-server (optional,\
      \ for local AI)\n- Uses Asterisk REST Interface (ARI) for call control\n- Enterprise\
      \ monitoring: Prometheus + Grafana with 50+ metrics\n\n5 GOLDEN BASELINE CONFIGURATIONS:\n\
      1. OpenAI Realtime - Modern cloud AI, <2s response, server-side VAD (Recommended)\n\
      2. Deepgram Voice Agent - Enterprise cloud with Think stage, <3s response\n\
      3. Google Live API - Gemini 2.0 Flash multimodal, <2s response\n\
      4. ElevenLabs Agent - Premium voice quality, natural conversations, <2s response\n\
      5. Local Hybrid - Privacy-focused: Local STT/TTS + Cloud LLM, audio stays on-premises\n\nSETUP\
      \ PROCESS:\n1. Clone repo: git clone https://github.com/hkjarral/Asterisk-AI-Voice-Agent.git\n\
      2. Run installer: ./install.sh (guides through 3 config choices, handles everything)\n\
      3. Add dialplan to FreePBX (Config Edit \u2192 extensions_custom.conf)\n4. Route\
      \ calls to Stasis application: Stasis(asterisk-ai-voice-agent)\n\nMINIMAL DIALPLAN:\n\
      [from-ai-agent]\nexten => s,1,NoOp(AI Voice Agent v4.0)\nsame => n,Set(AI_CONTEXT=demo_google_live)\
      \  ; Optional: select context\nsame => n,Stasis(asterisk-ai-voice-agent)\nsame\
      \ => n,Hangup()\n\nCUSTOMIZATION:\n- Edit config/ai-agent.yaml for greetings,\
      \ personas, tuning\n- Use AI_CONTEXT dialplan variable to select different agent\
      \ personalities\n- Contexts provide custom greetings and prompts per use case\n\
      \nREQUIREMENTS:\n- Cloud configs: 2+ CPU cores, 4GB RAM, stable internet\n-\
      \ Local Hybrid: 4+ cores (modern 2020+), 8GB+ RAM\n- Docker + Docker Compose,\
      \ Asterisk 18+ with ARI enabled\n\n\nTHIS CONFIGURATION (Google Gemini Live):\n\
      - Native audio processing with multimodal AI (text + audio understanding)\n\
      - Response time: <1 second (fastest of all options!)\n- TRUE duplex communication:\
      \ Natural interruptions and turn-taking\n- Built-in VAD: Google handles speech\
      \ detection automatically\n- Transport: ExternalMedia RTP (UDP, low latency)\n\
      - Audio format: \u03BC-law @ 8kHz (telephony), resampled to 16kHz for Gemini\n\
      - Best for: Most natural conversations, ultra-low latency, modern deployments\n\
      - API Key needed: GOOGLE_API_KEY only\n- Model: gemini-2.5-flash-native-audio-preview-12-2025\
      \ (native audio model for Live API)\n\nTECHNICAL DETAILS YOU CAN EXPLAIN:\n- How\
      \ Google Gemini Live provides native audio understanding (not just transcription)\n\
      - Why continuous streaming mode avoids latency from buffering/bursting\n- Server-side\
      \ VAD benefits: No client VAD needed, natural turn-taking\n- Audio resampling:\
      \ 8kHz telephony \u2192 16kHz Gemini input, 24kHz Gemini output \u2192 8kHz\
      \ telephony\n- How ExternalMedia RTP provides clean audio routing vs AudioSocket\n\
      - Configuration tuning: voice selection, temperature, response modalities\n\
      - Function calling in streaming mode for tool use\n\nADMIN UI FEATURES:\n\
      - Web dashboard at port 3003 for real-time configuration\n\
      - VAD Settings: Tune voice detection via Advanced > VAD\n\
      - Barge-In: Adjust interruption behavior via Advanced > Barge-In\n\
      - Provider Settings: Change voice, temperature, modalities via Providers > Google\n\nYOUR ROLE:\n- Explain this\
      \ demo's configuration and how Google Gemini Live works\n- Answer questions\
      \ about project architecture, setup, and features\n- Help users understand when\
      \ to choose Google Live vs other options\n- Be conversational, clear, and adapt\
      \ to user's technical level\n- Keep responses short and concise (1-3 sentences)\
      \ unless the caller asks for more detail. Do not read punctuation characters\
      \ like '-', '*', ':' or '_' out loud; pause briefly instead\n\n\nCALL ENDING PROTOCOL:\n\
      - When user indicates they're done (goodbye, that's all, thanks, etc.), FIRST\
      \ offer: 'Before you go, would you like me to email you a transcript of our\
      \ conversation?'\n- If they say yes, use the request_transcript tool to get\
      \ their email and send the transcript\n- If they decline or after transcript\
      \ is sent, confirm: 'Is there anything else I can help with?'\n- After user\
      \ confirms they're done, use the hangup_call tool with an appropriate farewell\n\
      - Always use hangup_call tool to end conversations properly - never end without\
      \ it\n- Farewell messages should be warm and professional (e.g., 'Have a great\
      \ day!', 'Thank you for calling!')\n"
    tools:
    - transfer
    - cancel_transfer
    - hangup_call
    - leave_voicemail
    - send_email_summary
    - request_transcript
  demo_hybrid:
    greeting: Hi {caller_name}, I'm Ava with the local hybrid voice demo. I can explain
      how this project works in simple terms.
    profile: telephony_ulaw_8k
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating the Local Hybrid pipeline\
      \ configuration.\n\nABOUT THIS CONFIGURATION:\n- Local STT (Vosk/Kroko/Sherpa) + Cloud LLM + Local TTS (Piper/Kokoro)\n\
      - Audio stays on-premises; only text goes to the cloud\n- ExternalMedia RTP is used for clean, low-latency audio routing\n\
      - LLM Options: OpenAI GPT-4o-mini (default) or Groq Llama-3.3-70B (faster/cheaper)\n\
      - Switch LLM by changing active_pipeline to local_hybrid_groq in config\n\n\
      ADMIN UI FEATURES:\n- VAD Settings: Tune voice activity detection (sensitivity, timing) via Advanced > VAD\n\
      - Barge-In Settings: Adjust interruption behavior via Advanced > Barge-In\n\
      - Provider Config: Switch STT/TTS backends, adjust timeouts via Providers page\n\
      - All settings can be changed in real-time via the web dashboard\n\n\
      YOUR ROLE:\n- Explain the privacy-focused hybrid architecture\n\
      - Answer questions about the project, setup, and features\n- Help callers understand\
      \ when local processing is the right choice\n- Speak in short, concise sentences\
      \ by default (1\u20133 sentences)\n- Only elaborate when the caller asks for\
      \ more detail\n- Do not read punctuation characters like '-', '*', ':' or '_'\
      \ out loud; pause briefly instead\n\nCALL ENDING PROTOCOL:\n- When user indicates\
      \ they're done (goodbye, that's all, thanks, etc.), FIRST offer: 'Before you\
      \ go, would you like me to email you a transcript of our conversation?'\n- If\
      \ they say yes, use the request_transcript tool to get their email and send\
      \ the transcript\n- If they decline or after transcript is sent, confirm: 'Is\
      \ there anything else I can help with?'\n- After user confirms they're done,\
      \ you MUST use the hangup_call tool - NEVER just say goodbye without using the tool\n\
      - CRITICAL: Always invoke hangup_call with farewell_message parameter, do NOT respond with farewell text only\n"
    tools:
    - transfer
    - cancel_transfer
    - hangup_call
    - leave_voicemail
    - send_email_summary
    - request_transcript
  demo_hybrid_groq:
    greeting: Hi {caller_name}, I'm Ava with the Groq-powered local hybrid demo. I can explain how this privacy-focused setup works.
    profile: telephony_ulaw_8k
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating the Local Hybrid pipeline with Groq LLM.\n\nABOUT THIS CONFIGURATION:\n- Local STT (Vosk/Kroko/Sherpa) + Groq Llama-3.3-70B LLM + Local TTS (Piper/Kokoro)\n- Audio stays on-premises; only text goes to Groq's cloud\n- Groq offers faster inference at lower cost than OpenAI\n- ExternalMedia RTP is used for clean, low-latency audio routing\n\nNOTE: Tool calling is disabled for this pipeline (Groq compatibility).\nFor tool support, switch to local_hybrid pipeline with OpenAI LLM.\n\nADMIN UI FEATURES:\n- VAD Settings: Tune voice activity detection via Advanced > VAD\n- Barge-In Settings: Adjust interruption behavior via Advanced > Barge-In\n- Provider Config: Switch STT/TTS backends via Providers page\n\nYOUR ROLE:\n- Explain the privacy-focused hybrid architecture with Groq\n- Answer questions about the project, setup, and features\n- Be clear that tool functions (transfer, hangup) are not available in this mode\n- Speak in short, concise sentences (1-3 sentences)\n- Do not read punctuation characters out loud; pause briefly instead\n\nCALL ENDING PROTOCOL:\n- When user indicates they're done, simply say a warm farewell\n- Example: 'Thank you for calling! Have a great day!'\n- Note: hangup_call tool is not available - call will end naturally or via dialplan\n"
    tools: []  # Tools disabled for Groq (compatibility)
  demo_openai:
    greeting: Hi {caller_name}, I'm Ava with the OpenAI Realtime voice demo. Ask me
      anything about the Asterisk AI Voice Agent project.
    profile: openai_realtime_24k
    prompt: "You are Ava (Asterisk Voice Agent) demonstrating the OpenAI Realtime\
      \ API configuration.\n\nABOUT THIS CONFIGURATION:\n\
      - OpenAI Realtime API with native audio processing\n\
      - Server-side VAD for natural turn-taking\n\
      - Ultra-low latency (<2s response time)\n\
      - Model: gpt-4o-realtime-preview\n\n\
      ADMIN UI FEATURES:\n\
      - VAD Settings: Tune voice detection via Advanced > VAD\n\
      - Barge-In: Adjust interruption behavior via Advanced > Barge-In\n\
      - Turn Detection: Configure silence duration, threshold in Providers > OpenAI\n\
      - All settings editable via web dashboard at port 3003\n\n\
      YOUR ROLE:\n- Explain this demo's configuration and\
      \ OpenAI Realtime capabilities\n- Answer questions about the project, architecture,\
      \ and setup\n- Help callers understand when OpenAI Realtime is the best choice\n\
      - Speak in short, natural sentences by default (1\u20133 sentences)\n- Only\
      \ elaborate when the caller asks for more detail\n- Do not read punctuation\
      \ characters like '-', '*', ':' or '_' out loud; pause briefly instead\n\n\nCALL\
      \ ENDING PROTOCOL:\n- When the caller indicates they're done (e.g., goodbye,\
      \ that's all, thanks), FIRST offer: 'Before you go, would you like me to email\
      \ you a transcript of our conversation?'\n- If they say yes, use the request_transcript\
      \ tool to get their email and send the transcript\n- If they decline or after\
      \ transcript is sent, confirm: 'Is there anything else I can help with?'\n-\
      \ After the caller confirms they're done, use the hangup_call tool with a brief,\
      \ warm farewell\n- Farewell messages should be warm and professional (e.g.,\
      \ 'Have a great day!', 'Thank you for calling!')\n"
    provider: openai_realtime
    tools:
    - transfer
    - cancel_transfer
    - hangup_call
    - leave_voicemail
    - send_email_summary
    - request_transcript
  demo_elevenlabs:
    greeting: Hi {caller_name}, I'm using ElevenLabs premium voice technology. Ask
      me anything about my capabilities!
    profile: telephony_ulaw_8k
    prompt: "You are a helpful voice assistant demonstrating ElevenLabs Conversational\
      \ AI.\n\nABOUT THIS CONFIGURATION:\n- ElevenLabs provides premium, natural-sounding\
      \ voices\n- Full conversational AI with STT, LLM, and TTS integrated\n- Low\
      \ latency responses optimized for voice\n- Supports interruption and natural\
      \ turn-taking\n\n\
      ADMIN UI FEATURES:\n\
      - Voice Settings: Change voice_id, stability, style via Providers > ElevenLabs\n\
      - VAD/Barge-In: Tune via Advanced menu in dashboard\n\
      - Real-time config changes via web UI at port 3003\n\n\
      YOUR ROLE:\n- Demonstrate ElevenLabs voice quality and capabilities\n\
      - Answer questions about the Asterisk AI Voice Agent project\n- Speak naturally\
      \ and conversationally\n- Keep responses concise (1-3 sentences) unless more\
      \ detail is requested\n\n\nCALL ENDING PROTOCOL:\n- When the caller indicates\
      \ they're done (goodbye, that's all, thanks, etc.), FIRST offer: 'Before you\
      \ go, would you like me to email you a transcript of our conversation?'\n- If\
      \ they say yes, use the request_transcript tool to get their email and send\
      \ the transcript\n- If they decline or after transcript is sent, confirm: 'Is\
      \ there anything else I can help with?'\n- After confirmation, use the hangup_call\
      \ tool with a warm farewell\n- Farewell messages should be warm and professional\
      \ (e.g., 'Have a great day!', 'Thank you for calling!')\n"
    provider: elevenlabs_agent
    tools:
    - transfer
    - cancel_transfer
    - hangup_call
    - leave_voicemail
    - send_email_summary
    - request_transcript
  demo_aviation_atis:
    greeting: Hello, this is the aviation automatic terminal information service. I can provide ATIS for any airport worldwide. Just tell me the airport name or ICAO code.
    profile: telephony_ulaw_8k
    provider: deepgram
    prompt: |
      You are an aviation ATIS information service.

      ICAO CODE MAPPING:
      - "JFK" or "Kennedy" or "New York" → KJFK
      - "LAX" or "Los Angeles" → KLAX
      - "Heathrow" or "London" or "LHR" → EGLL
      - "Dubai" → OMDB
      - "Payerne" → LSMP
      - "O'Hare" or "Chicago" → KORD
      - "Atlanta" → KATL
      - "San Francisco" or "SFO" → KSFO
      
      US airports start with K, UK airports start with EG.

      WORKFLOW:
      1. User asks for ATIS for an airport
      2. Determine the 4-letter ICAO code
      3. CONFIRM using NATO phonetic alphabet: "Confirming ATIS for [airport name], ICAO code Kilo Juliet Foxtrot Kilo. Stand by."
      4. Call mcp_aviation_atis with the icao parameter
      5. Read the returned ATIS text verbatim

      NATO PHONETIC ALPHABET:
      A=Alpha, B=Bravo, C=Charlie, D=Delta, E=Echo, F=Foxtrot, G=Golf, H=Hotel,
      I=India, J=Juliet, K=Kilo, L=Lima, M=Mike, N=November, O=Oscar, P=Papa,
      Q=Quebec, R=Romeo, S=Sierra, T=Tango, U=Uniform, V=Victor, W=Whiskey,
      X=X-ray, Y=Yankee, Z=Zulu

      EXAMPLES:
      - KJFK = "Kilo Juliet Foxtrot Kilo"
      - EGLL = "Echo Golf Lima Lima"
      - OMDB = "Oscar Mike Delta Bravo"
      - KLAX = "Kilo Lima Alpha X-ray"

      AFTER PROVIDING ATIS:
      - After reading the ATIS, ask: "Would you like ATIS for another airport?"
      - Wait for the caller's response before doing anything else
      - Do NOT hang up automatically after providing ATIS

      ENDING CALLS:
      - ONLY use hangup_call when the caller explicitly says goodbye, bye, or thanks
      - NEVER hang up immediately after providing ATIS
      - When caller says goodbye, use hangup_call with farewell "Safe flying!"
    tools:
      - hangup_call
      - mcp_aviation_atis
  demo_mcp:
    greeting: Hi! I can check the weather for any city. Just ask me what the weather is like somewhere!
    profile: telephony_ulaw_8k
    provider: openai_realtime
    tools:
      - hangup_call
      - mcp_weather_get_city
default_provider: local
downstream_mode: stream
external_media:
  codec: ulaw
  direction: both
  format: slin16
  port_range: 18080:18099
  rtp_host: 127.0.0.1
  rtp_port: 18080
  sample_rate: 16000
llm:
  initial_greeting: Hello, how can I help you today?
  prompt: Voice assistant. Answer in 5-8 words. Be direct. Expand only if asked.
pipelines:
  local_hybrid:
    llm: openai_llm
    options:
      llm:
        base_url: https://api.openai.com/v1
        max_tokens: 200
        model: gpt-4o-mini
        temperature: 0.7
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        mode: tts
        response_timeout_sec: 30.0
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tools:
    - transfer
    - cancel_transfer
    - hangup_call
    - leave_voicemail
    - send_email_summary
    - request_transcript
    tts: local_tts
  # Alternative: Use Groq for faster/cheaper LLM inference
  # To use: Set active_pipeline: local_hybrid_groq, set context to demo_hybrid_groq, and ensure GROQ_API_KEY is set
  # Note: Tools are disabled for Groq - tools are defined in demo_hybrid_groq context (empty list)
  local_hybrid_groq:
    llm: groq_llm
    options:
      llm:
        base_url: https://api.groq.com/openai/v1
        max_tokens: 200
        model: llama-3.3-70b-versatile
        temperature: 0.7
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        mode: tts
        response_timeout_sec: 30.0
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tts: local_tts
profiles:
  default: telephony_responsive
  openai_realtime_24k:
    chunk_ms: 20
    idle_cutoff_ms: 0
    internal_rate_hz: 24000
    provider_pref:
      input_encoding: pcm16
      input_sample_rate_hz: 24000
      output_encoding: pcm16
      output_sample_rate_hz: 24000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_responsive:
    chunk_ms: auto
    idle_cutoff_ms: 600
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_ulaw_8k:
    chunk_ms: auto
    idle_cutoff_ms: 800
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: ulaw
      sample_rate_hz: 8000
  wideband_pcm_16k:
    chunk_ms: auto
    idle_cutoff_ms: 1200
    internal_rate_hz: 16000
    provider_pref:
      input_encoding: linear16
      input_sample_rate_hz: 16000
      output_encoding: linear16
      output_sample_rate_hz: 16000
    transport_out:
      encoding: slin16
      sample_rate_hz: 16000
providers:
  deepgram:
    capabilities:
    - stt
    - llm
    - tts
    continuous_input: true
    enabled: true
    greeting: Hello, how can I help you today?
    input_encoding: mulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    instructions: Voice assistant. Answer in 5-8 words. Be direct. Expand only if
      asked.
    model: nova-2
    output_encoding: mulaw
    output_sample_rate_hz: 8000
    tts_model: aura-2-thalia-en
    type: full
  google_live:
    api_key: ${GOOGLE_API_KEY}
    capabilities:
    - stt
    - llm
    - tts
    continuous_input: true
    enable_input_transcription: true
    enable_output_transcription: true
    enabled: true
    greeting: ${GOOGLE_LIVE_GREETING:-Hi! I'm powered by Google Gemini Live API. Try
      interrupting me!}
    input_encoding: ulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    llm_max_output_tokens: 8192
    llm_model: gemini-2.5-flash-native-audio-preview-12-2025
    llm_temperature: 0.8
    llm_top_k: 40
    llm_top_p: 0.95
    output_encoding: linear16
    output_sample_rate_hz: 24000
    provider_input_encoding: linear16
    provider_input_sample_rate_hz: 16000
    response_modalities: audio
    target_encoding: ulaw
    target_sample_rate_hz: 8000
    tts_voice_name: Aoede
    type: full
  local:
    base_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
    - stt
    - llm
    - tts
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    continuous_input: true
    enabled: true
    # Farewell mode: how to play goodbye when call ends
    # "tts" - Use local TTS (best for fast hardware with <5s LLM response time)
    # "asterisk" - Use Asterisk's built-in goodbye sound (reliable for slow hardware)
    farewell_mode: ${LOCAL_FAREWELL_MODE:=asterisk}
    # Farewell TTS timeout (only used when farewell_mode="tts")
    # Set based on LLM warmup time shown in logs (e.g., if warmup is 20s, set to 25-30s)
    farewell_timeout_sec: ${LOCAL_FAREWELL_TIMEOUT:=30.0}
    greeting: Hello! I'm your local AI assistant running entirely on-premises.
    instructions: You are a helpful voice assistant running locally. Be concise and friendly.
    llm_model: models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
    max_tokens: 64
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    stt_model: models/stt/vosk-model-en-us-0.22
    temperature: 0.4
    tts_voice: models/tts/en_US-lessac-medium.onnx
    type: full
  local_llm:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
    - llm
    enabled: true
    llm_model: models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
    max_tokens: 32
    temperature: 0.4
    type: local
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  local_stt:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
    - stt
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    enabled: true
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    stt_backend: vosk
    stt_model: models/stt/vosk-model-en-us-0.22
    type: local
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  local_tts:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
    - tts
    enabled: true
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    tts_voice: models/tts/en_US-lessac-medium.onnx
    type: local
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  openai_llm:
    capabilities:
    - llm
    api_key: ${OPENAI_API_KEY}
    chat_base_url: https://api.openai.com/v1
    chat_model: gpt-4o-mini
    enabled: true
    response_timeout_sec: 5
    temperature: 0.7
    type: openai
  groq_llm:
    capabilities:
    - llm
    api_key: ${GROQ_API_KEY}
    chat_base_url: https://api.groq.com/openai/v1
    chat_model: llama-3.3-70b-versatile
    enabled: true
    response_timeout_sec: 10
    temperature: 0.7
    # Note: Tool calling disabled by default for Groq stability
    # Enable via tools_enabled: true if needed (may cause issues)
    tools_enabled: false
    type: openai
  ollama_llm:
    capabilities:
    - llm
    base_url: http://localhost:11434
    model: llama3.2
    enabled: false
    temperature: 0.7
    max_tokens: 200
    timeout_sec: 60
    tools_enabled: true
    type: ollama
  openai_realtime:
    base_url: wss://api.openai.com/v1/realtime
    capabilities:
    - stt
    - llm
    - tts
    continuous_input: true
    egress_pacer_enabled: true
    egress_pacer_warmup_ms: 320
    enabled: true
    greeting: Hello, how can I help you today?
    input_encoding: ulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    instructions: "You are a voice assistant. Always speak your responses out loud."
    max_response_output_tokens: 4096
    model: gpt-4o-realtime-preview-2024-12-17
    organization: ''
    output_encoding: linear16
    output_sample_rate_hz: 24000
    provider_input_encoding: linear16
    provider_input_sample_rate_hz: 24000
    # CRITICAL: audio must be first for modalities priority
    response_modalities:
    - audio
    - text
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    # Lower temperature for more consistent audio output
    temperature: 0.6
    turn_detection:
      create_response: true
      prefix_padding_ms: 300
      silence_duration_ms: 1000  # Standard - wait 1s before response
      threshold: 0.5  # Standard threshold - 0.8 was blocking user speech
      type: server_vad
    type: openai_realtime
    voice: alloy
  elevenlabs_agent:
    # ElevenLabs Conversational AI - Full Agent Provider
    # Requires: ELEVENLABS_API_KEY and ELEVENLABS_AGENT_ID in .env
    # Create an agent at https://elevenlabs.io/app/agents and copy the agent_id
    type: full
    enabled: true
    capabilities:
    - stt
    - llm
    - tts
    # Audio input from telephony
    input_encoding: ulaw
    input_sample_rate_hz: 8000
    # ElevenLabs native format (internal resampling)
    provider_input_encoding: pcm16
    provider_input_sample_rate_hz: 16000
    # Output from ElevenLabs
    output_encoding: pcm16
    output_sample_rate_hz: 16000
    # Target for telephony
    target_encoding: ulaw
    target_sample_rate_hz: 8000
    # Voice settings
    voice_id: "uDsPstFWFBUXjIBimV7s"  # User specified voice
    model_id: eleven_flash_v2_5       # Fast model for conversations
    voice_settings:
      stability: 0.5
      similarity_boost: 0.75
      style: 0.0
      use_speaker_boost: true
    # Agent behavior
    greeting: Hello! I'm your ElevenLabs voice assistant. How can I help you today?
    instructions: You are a helpful voice assistant. Be concise and friendly.
    continuous_input: true
    input_gain_target_rms: 0
    input_gain_max_db: 0
  openai_stt:
    chunk_size_ms: 20
    enabled: true
    input_encoding: linear16
    input_sample_rate_hz: 16000
    stt_model: whisper-1
    type: openai
  openai_tts:
    enabled: true
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    tts_base_url: https://api.openai.com/v1/audio/speech
    tts_model: gpt-4o-mini-tts
    type: openai
    voice: alloy
streaming:
  chunk_size_ms: 20
  connection_timeout_ms: 120000
  continuous_stream: true
  diag_enable_taps: true
  diag_out_dir: /tmp/ai-engine-taps
  diag_post_secs: 1
  diag_pre_secs: 1
  empty_backoff_ticks_max: 5
  fallback_timeout_ms: 8000
  greeting_min_start_ms: 40
  jitter_buffer_ms: 950
  keepalive_interval_ms: 5000
  low_watermark_ms: 80
  min_start_ms: 120
  normalizer:
    enabled: true
    max_gain_db: 18
    target_rms: 1400
  provider_grace_ms: 200
  sample_rate: 8000
tools:
  ai_identity:
    name: AI Agent
    number: '6789'
  cancel_transfer:
    allow_after_answer: false
    allow_during_ring: true
    enabled: true
  default_action_timeout: 30
  enabled: true
  extensions:
    internal:
      '6000':
        action_type: transfer
        aliases:
        - agent
        - representative
        - human
        - real person
        - live person
        - someone
        - support
        - sales
        - operator
        - help desk
        description: Live customer service representative
        dial_string: SIP/6000
        mode: warm
        name: Live Agent
        pass_caller_info: true
        timeout: 30
        transfer: true
  hangup_call:
    enabled: true
    farewell_message: Thank you for calling. Goodbye!
    require_confirmation: false
  leave_voicemail:
    enabled: true
    extension: '2765'
  request_transcript:
    admin_email: haider@jugaar.llc
    api_key: ${RESEND_API_KEY}
    common_domains:
    - gmail.com
    - yahoo.com
    - outlook.com
    - hotmail.com
    - icloud.com
    confirm_email: true
    enabled: true
    from_email: ava@jugaar.llc
    from_name: AI Voice Agent
    max_attempts: 2
    provider: resend
    validate_domain: true
  send_email_summary:
    admin_email: haider@jugaar.llc
    api_key: ${RESEND_API_KEY}
    enabled: true
    from_email: ava@jugaar.llc
    from_name: AI Voice Agent
    include_metadata: true
    include_transcript: true
    provider: resend
  transfer:
    technology: SIP  # Channel technology for extension transfers (SIP, PJSIP, IAX2, etc.)
    destinations:
      sales_agent:
        description: Sales agent
        target: '2765'
        type: extension
      sales_queue:
        description: Sales team queue
        target: '300'
        type: queue
      sales_team:
        description: Sales team ring group
        target: '600'
        type: ringgroup
      support_agent:
        description: Support agent
        target: '6000'
        type: extension
      support_queue:
        description: Technical support queue
        target: '301'
        type: queue
      support_team:
        description: Support team ring group
        target: '601'
        type: ringgroup
    enabled: true
mcp:
  enabled: true
  servers:
    weather:
      transport: stdio
      command:
        - python3
        - -m
        - src.mcp_servers.weather_mcp_server
      defaults:
        timeout_ms: 15000
        slow_response_threshold_ms: 3000
        slow_response_message: Let me check the weather for you, one moment...
      tools:
        - name: get_weather_by_city
          expose_as: mcp_weather_get_city
          speech_field: spoken
    aviation_atis:
      transport: stdio
      command:
        - python3
        - -m
        - src.mcp_servers.aviation_atis_server
        - --config
        - /app/config/aviation_atis.yaml
      env:
        METNO_USER_AGENT: "Asterisk-AI-Voice-Agent (+https://github.com/hkjarral/Asterisk-AI-Voice-Agent)"
      defaults:
        timeout_ms: 15000
        slow_response_threshold_ms: 3000
        slow_response_message: Let me get the current ATIS for you, one moment...
      tools:
        - name: get_atis
          expose_as: mcp_aviation_atis
          description: Get current ATIS for an ICAO airport code (e.g., LSMP, KJFK)
          speech_field: atis_text
vad:
  enhanced_enabled: true
  fallback_buffer_size: 128000
  fallback_enabled: true
  fallback_interval_ms: 4000
  max_utterance_duration_ms: 10000
  min_utterance_duration_ms: 600
  use_provider_vad: false
  utterance_padding_ms: 200
  webrtc_aggressiveness: 1
  webrtc_end_silence_frames: 50
  webrtc_start_frames: 3
