# ═══════════════════════════════════════════════════════════════════════════
# Asterisk AI Voice Agent - Environment Configuration
# ═══════════════════════════════════════════════════════════════════════════
# Copy this file to .env and configure for your environment.
# 
# IMPORTANT: This file contains SECRETS and ENVIRONMENT-SPECIFIC settings only.
# For application behavior (audio transport, pipelines, barge-in, etc.),
# edit config/ai-agent.yaml instead.

COMPOSE_PROJECT_NAME=asterisk-ai-voice-agent

# ═══════════════════════════════════════════════════════════════════════════
# REQUIRED: Asterisk ARI Connection
# ═══════════════════════════════════════════════════════════════════════════

# ASTERISK_HOST: How ai-engine connects to Asterisk ARI
# - Use IP address (127.0.0.1) for local Asterisk
# - Use hostname (asterisk.example.com) for remote Asterisk
# NOTE: When using hostname, you MUST set allowed_remote_hosts in ai-agent.yaml
#       or via the Setup Wizard for RTP security
ASTERISK_HOST=127.0.0.1

# ARI Port (default: 8088, some setups use custom ports like 20071)
ASTERISK_ARI_PORT=8088

# ARI Scheme (default: http, use https for secure/WSS connections)
# - http: Uses ws:// for WebSocket (local/trusted networks)
# - https: Uses wss:// for WebSocket (remote/internet connections)
# ASTERISK_ARI_SCHEME=http

# SSL Certificate Verification (default: true)
# Set to false to skip SSL certificate verification for self-signed certs
# or when certificate doesn't match hostname/IP
# ASTERISK_ARI_SSL_VERIFY=true

# ARI Credentials (SECRETS - keep in .env, never commit to git)
# Create in FreePBX: Settings → Asterisk REST Interface Users
ASTERISK_ARI_USERNAME=asterisk
ASTERISK_ARI_PASSWORD=asterisk

# Asterisk User/Group IDs (for container permission alignment)
# Detect with: id -u asterisk && id -g asterisk
# Defaults to 995 (FreePBX standard) - adjust for your system
# ASTERISK_UID=995
# ASTERISK_GID=995

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL: Rootless Docker / Admin UI Docker Socket
# ═══════════════════════════════════════════════════════════════════════════
# If your host uses rootless Docker, Admin UI must mount the rootless socket.
# Example:
# DOCKER_SOCK=/run/user/1000/docker.sock
#
# Tier 3 / Best-effort hosts (Docker Desktop, Podman, unsupported distros):
# If the Admin UI shows AI Engine / Local AI Server as "unreachable" while containers
# are running, set explicit health probe URLs that are reachable from the admin-ui container:
#
# HEALTH_CHECK_AI_ENGINE_URL=http://ai_engine:15000/health
# HEALTH_CHECK_LOCAL_AI_URL=ws://local_ai_server:8765
#
# If you want Local AI Server to be reachable from other containers (bridge networking),
# it must bind non-loopback. This is security-sensitive and requires auth:
#
# LOCAL_WS_HOST=0.0.0.0
# LOCAL_WS_AUTH_TOKEN=change-me  # REQUIRED when LOCAL_WS_HOST is non-loopback

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL (v5.0.0): Outbound Campaign Dialer (Alpha)
# ═══════════════════════════════════════════════════════════════════════════
#
# Outbound calling is managed from Admin UI → Call Scheduling.
# It assumes your trunk(s) and outbound routes are already configured in Asterisk/FreePBX.
#
# Extension identity used for FreePBX routing (sets AMPUSER + CALLERID(num) on originate):
# AAVA_OUTBOUND_EXTENSION_IDENTITY=6789
#
# Dialplan context used for the AMD hop (engine uses ARI continueInDialplan):
# AAVA_OUTBOUND_AMD_CONTEXT=aava-outbound-amd
#
# Shared media dir for outbound recordings (voicemail drop + consent prompt):
# AAVA_MEDIA_DIR=/mnt/asterisk_media/ai-generated
#
# Upload size limit for voicemail/consent recordings (bytes). WAV is auto-converted to 8kHz μ-law:
# AAVA_VM_UPLOAD_MAX_BYTES=12582912
#
# Optional server timezone override for the Admin UI clock (IANA TZ string):
# AAVA_SERVER_TIMEZONE=UTC

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL: NAT / Hybrid Network Configuration (Milestone 23)
# ═══════════════════════════════════════════════════════════════════════════
# Use these when AI engine is behind NAT and Asterisk is remote.
# Set to the IP address that Asterisk can reach (VPN IP, public IP, LAN IP).
#
# For AudioSocket transport:
# AUDIOSOCKET_ADVERTISE_HOST=10.8.0.5
#
# For ExternalMedia RTP transport:
# EXTERNAL_MEDIA_ADVERTISE_HOST=10.8.0.5

# ═══════════════════════════════════════════════════════════════════════════
# REQUIRED: AI Provider API Keys (SECRETS)
# ═══════════════════════════════════════════════════════════════════════════
# Get keys at:
#   OpenAI: https://platform.openai.com/api-keys
#   Deepgram: https://console.deepgram.com/
#   Google Cloud: https://console.cloud.google.com/apis/credentials

# API keys are set by the Setup Wizard or manually
# Leave empty until you configure them - providers will show "Not Ready" until set
OPENAI_API_KEY=
DEEPGRAM_API_KEY=
GOOGLE_API_KEY=
ELEVENLABS_API_KEY=

# ═══════════════════════════════════════════════════════════════════════════
# REQUIRED (Production): Admin UI Auth (SECRETS)
# ═══════════════════════════════════════════════════════════════════════════
# JWT secret used by the Admin UI backend to sign auth tokens.
# IMPORTANT: This will be auto-generated by preflight.sh or install.sh.
# If running manually, generate with: openssl rand -hex 32
#
# WARNING: If left empty, Admin UI will use an ephemeral secret that changes
# on every restart, logging out all users. Always run preflight.sh first!
JWT_SECRET=

# Admin UI bind controls (advanced).
# Default is remote-accessible for first-run usability. For production hardening,
# consider binding to localhost and placing a reverse proxy/VPN in front.
# UVICORN_HOST=0.0.0.0
# UVICORN_PORT=3003

# Option 2: Service Account (recommended for production)
# Create service account at: https://console.cloud.google.com/iam-admin/serviceaccounts
# Download JSON key and set the full path below
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
#
# Required APIs to enable:
#   - Cloud Speech-to-Text API (for STT)
#   - Cloud Text-to-Speech API (for TTS)
#   - Generative Language API (for Gemini LLM)
#   - Gemini Live API (for google_live real-time agent)
#
# IAM Roles needed:
#   - roles/speech.client (for STT)
#   - roles/texttospeech.client (for TTS)
#   - roles/generativelanguage.user (for Gemini LLM)
#   - roles/generativelanguage.liveapi.user (for Gemini Live API)
#
# For Google Live API (google_live provider):
#   - Use GOOGLE_API_KEY for direct API access
#   - Or GOOGLE_APPLICATION_CREDENTIALS for service account
#   - Live API enables real-time bidirectional streaming with barge-in

# ═══════════════════════════════════════════════════════════════════════════
# System Configuration
# ═══════════════════════════════════════════════════════════════════════════

# Timezone for consistent timestamp display in logs, call history, and Admin UI.
# Should match your Asterisk server timezone for accurate call timing.
# See: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
TZ=America/Phoenix

# ═══════════════════════════════════════════════════════════════════════════
# AI Assistant Configuration (User Preferences)
# ═══════════════════════════════════════════════════════════════════════════

GREETING="Hello, how can I help you today?"
AI_ROLE="You are a concise and helpful voice assistant."

# ═══════════════════════════════════════════════════════════════════════════
# AI Engine Logging Configuration (Environment-Specific)
# ═══════════════════════════════════════════════════════════════════════════
# These settings control logging for the main ai-engine container
# Adjust these per environment (dev=debug, prod=info)

LOG_LEVEL=info               # AI Engine: debug|info|warning|error|critical
LOG_FORMAT=console           # AI Engine: console (colored) | json (for log aggregation)
LOG_COLOR=1                  # AI Engine: console only: 1=colored, 0=plain
LOG_SHOW_TRACEBACKS=auto     # AI Engine: auto|always|never
STREAMING_LOG_LEVEL=info     # AI Engine: Audio pipeline logging verbosity

# ═══════════════════════════════════════════════════════════════════════════
# Admin UI Runtime (Optional)
# ═══════════════════════════════════════════════════════════════════════════
# Comma-separated list of allowed origins for the Admin UI API CORS policy.
# Defaults to http://localhost:3003 and http://127.0.0.1:3003.
# Set to "*" only for advanced debugging; credentials will be disabled if "*".
# ADMIN_UI_CORS_ORIGINS=http://localhost:3003,http://your-domain.example

# Optional: File logging (Docker logs usually sufficient)
# LOG_TO_FILE=0
# LOG_FILE_PATH=/mnt/asterisk_media/ai-engine.log

# ═══════════════════════════════════════════════════════════════════════════
# Local AI Server Connection (Optional - for local_hybrid pipeline)
# ═══════════════════════════════════════════════════════════════════════════
# Only used if you enable local_hybrid pipeline in ai-agent.yaml

LOCAL_WS_URL=ws://127.0.0.1:8765
# local-ai-server bind controls (server-side). Only needed if you want to bind
# to a different interface/port; keep LOCAL_WS_URL in sync if you change PORT.
# SECURITY: prefer 127.0.0.1 unless you explicitly need LAN/WAN access.
LOCAL_WS_HOST=127.0.0.1
LOCAL_WS_PORT=8765
# Optional auth token for local-ai-server WebSocket. If set here, also set
# providers.local*.auth_token to ${LOCAL_WS_AUTH_TOKEN} in ai-agent.yaml.
LOCAL_WS_AUTH_TOKEN=
LOCAL_WS_CONNECT_TIMEOUT=2.0
LOCAL_WS_RESPONSE_TIMEOUT=5.0
LOCAL_WS_CHUNK_MS=320

# ═══════════════════════════════════════════════════════════════════════════
# Local AI Server Logging (local-ai-server container only)
# ═══════════════════════════════════════════════════════════════════════════
# These settings control logging for the local-ai-server container.
# Only applies when using local_hybrid, local_only, or hybrid_support pipelines.
#
# IMPORTANT: After changing these values, you must recreate the container:
#   docker compose down local-ai-server
#   docker compose up -d local-ai-server
# A simple restart (docker compose restart) will NOT pick up .env changes!

# Log level for the local-ai-server process
# Values: DEBUG | INFO | WARNING | ERROR | CRITICAL
# - DEBUG:   All logs including WebSocket messages, audio routing, model loading
# - INFO:    Normal operation logs (recommended for production)
# - WARNING: Only warnings and errors
# - ERROR:   Only errors
LOCAL_LOG_LEVEL=INFO

# Verbose audio flow debugging (separate from log level)
# Set to 1 to enable detailed audio processing logs:
# - "FEEDING VOSK" messages with byte counts
# - RMS/energy calculations for each audio chunk
# - Audio buffer states and routing decisions
# WARNING: Creates very high log volume - only enable for troubleshooting!
LOCAL_DEBUG=0

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - Runtime Mode
# ─────────────────────────────────────────────────────────────
# Default is "full" (preloads STT + LLM + TTS).
# Use "minimal" to skip LLM preload for faster startup and lower memory.
LOCAL_AI_MODE=full            # full | minimal

# Local AI Server - STT Backend Selection
# ─────────────────────────────────────────────────────────────
# Choose STT backend implementation. Default is vosk.
LOCAL_STT_BACKEND=vosk        # vosk | kroko | sherpa | faster_whisper

# Sherpa-onnx STT Settings (only used when LOCAL_STT_BACKEND=sherpa)
# ─────────────────────────────────────────────────────────────
# Local streaming ASR using sherpa-onnx (no server needed)
#SHERPA_MODEL_PATH=/app/models/stt/sherpa-onnx-streaming-zipformer-en-2023-06-26

# Faster-Whisper STT Settings (only used when LOCAL_STT_BACKEND=faster_whisper)
# ─────────────────────────────────────────────────────────────
# High-accuracy Whisper-based ASR using CTranslate2 optimization
# Requires: docker build --build-arg INCLUDE_FASTER_WHISPER=true
# Models auto-download from HuggingFace on first use
#FASTER_WHISPER_MODEL=base     # Model size: tiny, base, small, medium, large-v2, large-v3
#FASTER_WHISPER_DEVICE=cpu     # Device: cpu, cuda, or auto
#FASTER_WHISPER_COMPUTE_TYPE=int8  # Compute type: int8, float16, float32
#FASTER_WHISPER_LANGUAGE=en    # Language code (e.g., en, es, fr, de)

# Kroko ASR Settings (only used when LOCAL_STT_BACKEND=kroko)
# ─────────────────────────────────────────────────────────────
# Option 1: Hosted API (easiest - no model download required)
#   Get API key at: https://app.kroko.ai/
KROKO_URL=wss://app.kroko.ai/api/v1/transcripts/streaming
KROKO_API_KEY=                # Your Kroko API key (for hosted API)

# Option 2: On-premise server (run your own Kroko ONNX server)
#   Download models: https://huggingface.co/Banafo/Kroko-ASR
#KROKO_URL=ws://localhost:6006
#KROKO_API_KEY=               # Not needed for on-premise

# Option 3: Embedded mode (Kroko server runs inside local-ai-server container)
#   Requires building with: docker build --build-arg INCLUDE_KROKO_EMBEDDED=true
#KROKO_EMBEDDED=1
#KROKO_MODEL_PATH=/app/models/kroko/kroko-en-v1.0.onnx
#KROKO_PORT=6006

# Language code for Kroko (see https://docs.kroko.ai/languages/)
KROKO_LANGUAGE=en-US

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - TTS Backend Selection (AAVA-95)
# ───────────────────────────────────────────────────────────────────────────
# Choose between Piper (default) or Kokoro for text-to-speech
# Kokoro offers: high-quality 82M param model, multi-voice, Apache licensed
# Docs: https://huggingface.co/hexgrad/Kokoro-82M

LOCAL_TTS_BACKEND=piper       # TTS backend: piper (default), kokoro, or melotts

# MeloTTS Settings (only used when LOCAL_TTS_BACKEND=melotts)
# ─────────────────────────────────────────────────────────────
# Lightweight, CPU-optimized TTS with multiple English accents
# Requires: docker build --build-arg INCLUDE_MELOTTS=true
#MELOTTS_VOICE=EN-US          # Voice: EN-US, EN-BR (British), EN-AU, EN-IN (India), EN-Default
#MELOTTS_DEVICE=cpu           # Device: cpu or cuda
#MELOTTS_SPEED=1.0            # Speech speed (1.0 = normal)

# Kokoro TTS Settings (only used when LOCAL_TTS_BACKEND=kokoro)
# ─────────────────────────────────────────────────────────────
# Model files are downloaded by the setup wizard to /app/models/tts/kokoro/
# Voices: af_heart, af_bella, am_adam, am_michael (see VOICES.md)
#KOKORO_MODEL_PATH=/app/models/tts/kokoro
#KOKORO_VOICE=af_heart        # Default voice
#KOKORO_LANG=a                # 'a' = American English

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - Model Paths (Set by Setup Wizard or Dashboard)
# ───────────────────────────────────────────────────────────────────────────
# These paths point to downloaded models in /app/models/ (container path)
# Models are downloaded via Setup Wizard or Models Page in Admin UI
# You can switch models at runtime via Dashboard without container restart

#LOCAL_STT_MODEL_PATH=/app/models/stt/vosk-model-en-us-0.22
#LOCAL_LLM_MODEL_PATH=/app/models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
#LOCAL_TTS_MODEL_PATH=/app/models/tts/en_US-lessac-medium.onnx

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - LLM Performance Tuning
# ───────────────────────────────────────────────────────────────────────────
# Tune these for your hardware. Defaults are optimized for 4-8 core CPUs.
# Higher values = better quality but slower inference

#LOCAL_LLM_THREADS=16          # CPU threads for inference (default: min(16, cpu_count))
#LOCAL_LLM_CONTEXT=768         # Context window size (lower = faster, 512-2048)
#LOCAL_LLM_BATCH=256           # Batch size for prompt processing (128-512)
#LOCAL_LLM_MAX_TOKENS=48       # Max tokens per response (32-128 for voice)
#LOCAL_LLM_TEMPERATURE=0.2     # Response creativity (0.1-0.5 for consistency)
#LOCAL_LLM_TOP_P=0.85          # Nucleus sampling (0.8-0.95)
#LOCAL_LLM_REPEAT_PENALTY=1.05 # Repetition penalty (1.0-1.2)
#LOCAL_LLM_USE_MLOCK=0         # Lock model in RAM (1=yes, requires privileges)
#LOCAL_LLM_INFER_TIMEOUT_SEC=30 # Max seconds for LLM inference

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - GPU Acceleration (NVIDIA CUDA)
# ───────────────────────────────────────────────────────────────────────────
# Offload LLM layers to GPU for faster inference (requires NVIDIA GPU + CUDA)
#
# AAVA-140: GPU detection is now handled by preflight.sh
# Run ./preflight.sh to auto-detect GPU and set GPU_AVAILABLE below

# GPU_AVAILABLE: Auto-detected by preflight.sh (do not set manually)
# - true:  NVIDIA GPU detected on host
# - false: No GPU detected or nvidia-smi not found
# This is used by Admin UI wizard for tier detection without needing GPU passthrough
#GPU_AVAILABLE=false

LOCAL_LLM_GPU_LAYERS=0        # GPU layer offloading:
                              #   0  = CPU only (default, no GPU required)
                              #   -1 = Auto-detect (use GPU if CUDA available)
                              #   N  = Offload N layers to GPU (e.g., 35)
                              # Tip: Start with 35 layers, adjust based on VRAM

# To enable GPU for LLM inference (optional, faster responses):
# 1. Run ./preflight.sh (auto-detects GPU, sets GPU_AVAILABLE in .env)
#    - Setup Wizard will detect GPU automatically via this env var
#    - No workflow changes needed for detection!
# 2. Install NVIDIA Container Toolkit if prompted by preflight.sh
# 3. Set LOCAL_LLM_GPU_LAYERS=-1 (or specific layer count like 35)
# 4. Start local-ai-server with GPU override:
#    docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d local-ai-server
#
# Docs: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL: Monitoring & Email (SECRETS)
# ═══════════════════════════════════════════════════════════════════════════
# Get API key at https://resend.com
# Configure email recipients in config/ai-agent.yaml under monitoring.email

RESEND_API_KEY=

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL: Health Endpoint (Environment-Specific)
# ═══════════════════════════════════════════════════════════════════════════
# For external monitoring tools (Prometheus, etc.)

# HEALTH_BIND_HOST=127.0.0.1  # Use 0.0.0.0 for remote monitoring
# HEALTH_BIND_PORT=15000

# SECURITY: Required for remote access to sensitive endpoints (/reload, /mcp/test/*)
# Generate with: openssl rand -hex 32
# HEALTH_API_TOKEN=

# ═══════════════════════════════════════════════════════════════════════════
# DIAGNOSTIC: Audio Debugging (Troubleshooting Only)
# ═══════════════════════════════════════════════════════════════════════════
# DO NOT enable in production - creates WAV file taps for analysis
# See docs/TROUBLESHOOTING.md for usage

DIAG_ENABLE_TAPS=false
# DIAG_TAP_PRE_SECS=1
# DIAG_TAP_POST_SECS=1
# DIAG_TAP_OUTPUT_DIR=/tmp/ai-engine-taps
# DIAG_EGRESS_SWAP_MODE=none
# DIAG_EGRESS_FORCE_MULAW=false
# DIAG_ATTACK_MS=0

# ═══════════════════════════════════════════════════════════════════════════
# For Application Behavior Configuration, edit config/ai-agent.yaml:
# ═══════════════════════════════════════════════════════════════════════════
# ✅ Audio transport mode (audiosocket vs externalmedia)
# ✅ Downstream playback mode (stream vs file)
# ✅ Pipelines and providers
# ✅ Barge-in settings
# ✅ VAD configuration
# ✅ AudioSocket/ExternalMedia settings
#
# Advanced environment overrides (optional):
# - AUDIO_TRANSPORT           # Override audio_transport from YAML
# - DOWNSTREAM_MODE           # Override downstream_mode from YAML
# - AUDIOSOCKET_HOST          # Override audiosocket.host
# - AUDIOSOCKET_PORT          # Override audiosocket.port
# - AUDIOSOCKET_FORMAT        # Override audiosocket.format
# - EXTERNAL_MEDIA_RTP_HOST   # Override external_media.rtp_host
# - AST_MEDIA_DIR             # Override fallback media directory for generated audio
# - ASTERISK_GID              # GID of asterisk group on host (default: 995) - auto-detected by preflight.sh
#                             # Used at build time to add container user to asterisk group
#                             # Run `id asterisk` to find your system's GID
#
# Restart after changing .env: docker-compose down && docker-compose up -d
# Restart after changing YAML: docker-compose restart ai-engine

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL: Call History (Milestone 21)
# ═══════════════════════════════════════════════════════════════════════════
# Enable call history persistence for debugging and analytics

# Enable/disable call history recording
CALL_HISTORY_ENABLED=true

# Retention period in days (0 = unlimited, keep forever)
CALL_HISTORY_RETENTION_DAYS=0

# Database file path (relative to project root or absolute)
CALL_HISTORY_DB_PATH=data/call_history.db
