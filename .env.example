# ═══════════════════════════════════════════════════════════════════════════
# Asterisk AI Voice Agent - Environment Configuration
# ═══════════════════════════════════════════════════════════════════════════
# Copy this file to .env and configure for your environment.
# 
# IMPORTANT: This file contains SECRETS and ENVIRONMENT-SPECIFIC settings only.
# For application behavior (audio transport, pipelines, barge-in, etc.),
# edit config/ai-agent.yaml instead.

# ═══════════════════════════════════════════════════════════════════════════
# REQUIRED: Asterisk ARI Connection
# ═══════════════════════════════════════════════════════════════════════════

# ASTERISK_HOST: How ai-engine connects to Asterisk ARI
# This is environment-specific (differs between dev/staging/prod)
ASTERISK_HOST=127.0.0.1

# ARI Credentials (SECRETS - keep in .env, never commit to git)
# Create in FreePBX: Settings → Asterisk REST Interface Users
ASTERISK_ARI_USERNAME=asterisk
ASTERISK_ARI_PASSWORD=asterisk

# Asterisk User/Group IDs (for container permission alignment)
# Detect with: id -u asterisk && id -g asterisk
# Defaults to 995 (FreePBX standard) - adjust for your system
# ASTERISK_UID=995
# ASTERISK_GID=995

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL: Rootless Docker / Admin UI Docker Socket
# ═══════════════════════════════════════════════════════════════════════════
# If your host uses rootless Docker, Admin UI must mount the rootless socket.
# Example:
# DOCKER_SOCK=/run/user/1000/docker.sock

# ═══════════════════════════════════════════════════════════════════════════
# REQUIRED: AI Provider API Keys (SECRETS)
# ═══════════════════════════════════════════════════════════════════════════
# Get keys at:
#   OpenAI: https://platform.openai.com/api-keys
#   Deepgram: https://console.deepgram.com/
#   Google Cloud: https://console.cloud.google.com/apis/credentials

# API keys are set by the Setup Wizard or manually
# Leave empty until you configure them - providers will show "Not Ready" until set
OPENAI_API_KEY=
DEEPGRAM_API_KEY=
GOOGLE_API_KEY=
ELEVENLABS_API_KEY=

# ═══════════════════════════════════════════════════════════════════════════
# REQUIRED (Production): Admin UI Auth (SECRETS)
# ═══════════════════════════════════════════════════════════════════════════
# JWT secret used by the Admin UI backend to sign auth tokens.
# IMPORTANT: Set this to a long, random value in production.
JWT_SECRET=change-me-please

# Option 2: Service Account (recommended for production)
# Create service account at: https://console.cloud.google.com/iam-admin/serviceaccounts
# Download JSON key and set the full path below
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
#
# Required APIs to enable:
#   - Cloud Speech-to-Text API (for STT)
#   - Cloud Text-to-Speech API (for TTS)
#   - Generative Language API (for Gemini LLM)
#   - Gemini Live API (for google_live real-time agent)
#
# IAM Roles needed:
#   - roles/speech.client (for STT)
#   - roles/texttospeech.client (for TTS)
#   - roles/generativelanguage.user (for Gemini LLM)
#   - roles/generativelanguage.liveapi.user (for Gemini Live API)
#
# For Google Live API (google_live provider):
#   - Use GOOGLE_API_KEY for direct API access
#   - Or GOOGLE_APPLICATION_CREDENTIALS for service account
#   - Live API enables real-time bidirectional streaming with barge-in

# ═══════════════════════════════════════════════════════════════════════════
# AI Assistant Configuration (User Preferences)
# ═══════════════════════════════════════════════════════════════════════════

GREETING="Hello, how can I help you today?"
AI_ROLE="You are a concise and helpful voice assistant."

# ═══════════════════════════════════════════════════════════════════════════
# AI Engine Logging Configuration (Environment-Specific)
# ═══════════════════════════════════════════════════════════════════════════
# These settings control logging for the main ai-engine container
# Adjust these per environment (dev=debug, prod=info)

LOG_LEVEL=info               # AI Engine: debug|info|warning|error|critical
LOG_FORMAT=console           # AI Engine: console (colored) | json (for log aggregation)
LOG_COLOR=1                  # AI Engine: console only: 1=colored, 0=plain
LOG_SHOW_TRACEBACKS=auto     # AI Engine: auto|always|never
STREAMING_LOG_LEVEL=info     # AI Engine: Audio pipeline logging verbosity

# ═══════════════════════════════════════════════════════════════════════════
# Admin UI Runtime (Optional)
# ═══════════════════════════════════════════════════════════════════════════
# Comma-separated list of allowed origins for the Admin UI API CORS policy.
# Defaults to http://localhost:3003 and http://127.0.0.1:3003.
# Set to "*" only for advanced debugging; credentials will be disabled if "*".
# ADMIN_UI_CORS_ORIGINS=http://localhost:3003,http://your-domain.example

# Optional: File logging (Docker logs usually sufficient)
# LOG_TO_FILE=0
# LOG_FILE_PATH=/mnt/asterisk_media/ai-engine.log

# ═══════════════════════════════════════════════════════════════════════════
# Local AI Server Connection (Optional - for local_hybrid pipeline)
# ═══════════════════════════════════════════════════════════════════════════
# Only used if you enable local_hybrid pipeline in ai-agent.yaml

LOCAL_WS_URL=ws://127.0.0.1:8765
# local-ai-server bind controls (server-side). Only needed if you want to bind
# to a different interface/port; keep LOCAL_WS_URL in sync if you change PORT.
# SECURITY: prefer 127.0.0.1 unless you explicitly need LAN/WAN access.
LOCAL_WS_HOST=127.0.0.1
LOCAL_WS_PORT=8765
# Optional auth token for local-ai-server WebSocket. If set here, also set
# providers.local*.auth_token to ${LOCAL_WS_AUTH_TOKEN} in ai-agent.yaml.
LOCAL_WS_AUTH_TOKEN=
LOCAL_WS_CONNECT_TIMEOUT=2.0
LOCAL_WS_RESPONSE_TIMEOUT=5.0
LOCAL_WS_CHUNK_MS=320

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server Logging (local-ai-server container only)
# ───────────────────────────────────────────────────────────────────────────
# These settings control logging for the local-ai-server container
# Only applies when using local_hybrid or local_only pipelines

LOCAL_LOG_LEVEL=INFO          # Local AI Server: DEBUG|INFO|WARNING|ERROR (default: INFO)
LOCAL_DEBUG=0                 # Local AI Server: 1=Enable verbose audio flow logs
                              # (FEEDING VOSK, RMS calculation, audio routing, etc.)
                              # WARNING: Only enable for troubleshooting - creates high log volume
                              # Production: Keep at 0 for clean logs

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - STT Backend Selection (AAVA-92)
# ───────────────────────────────────────────────────────────────────────────
# Choose between Vosk (default) or Kroko ASR for speech-to-text
# Kroko offers: true low-latency streaming, no hallucination, 12+ languages
# Docs: https://docs.kroko.ai/

LOCAL_STT_BACKEND=vosk        # STT backend: vosk, kroko, or sherpa

# Sherpa-onnx STT Settings (only used when LOCAL_STT_BACKEND=sherpa)
# ─────────────────────────────────────────────────────────────
# Local streaming ASR using sherpa-onnx (no server needed)
#SHERPA_MODEL_PATH=/app/models/stt/sherpa-onnx-streaming-zipformer-en-2023-06-26

# Kroko ASR Settings (only used when LOCAL_STT_BACKEND=kroko)
# ─────────────────────────────────────────────────────────────
# Option 1: Hosted API (easiest - no model download required)
#   Get API key at: https://app.kroko.ai/
KROKO_URL=wss://app.kroko.ai/api/v1/transcripts/streaming
KROKO_API_KEY=                # Your Kroko API key (for hosted API)

# Option 2: On-premise server (run your own Kroko ONNX server)
#   Download models: https://huggingface.co/Banafo/Kroko-ASR
#KROKO_URL=ws://localhost:6006
#KROKO_API_KEY=               # Not needed for on-premise

# Option 3: Embedded mode (Kroko server runs inside local-ai-server container)
#   Requires building with: docker build --build-arg INCLUDE_KROKO_EMBEDDED=true
#KROKO_EMBEDDED=1
#KROKO_MODEL_PATH=/app/models/kroko/kroko-en-v1.0.onnx
#KROKO_PORT=6006

# Language code for Kroko (see https://docs.kroko.ai/languages/)
KROKO_LANGUAGE=en-US

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - TTS Backend Selection (AAVA-95)
# ───────────────────────────────────────────────────────────────────────────
# Choose between Piper (default) or Kokoro for text-to-speech
# Kokoro offers: high-quality 82M param model, multi-voice, Apache licensed
# Docs: https://huggingface.co/hexgrad/Kokoro-82M

LOCAL_TTS_BACKEND=piper       # TTS backend: piper (default) or kokoro

# Kokoro TTS Settings (only used when LOCAL_TTS_BACKEND=kokoro)
# ─────────────────────────────────────────────────────────────
# Model files are downloaded by the setup wizard to /app/models/tts/kokoro/
# Voices: af_heart, af_bella, am_adam, am_michael (see VOICES.md)
#KOKORO_MODEL_PATH=/app/models/tts/kokoro
#KOKORO_VOICE=af_heart        # Default voice
#KOKORO_LANG=a                # 'a' = American English

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - Model Paths (Set by Setup Wizard or Dashboard)
# ───────────────────────────────────────────────────────────────────────────
# These paths point to downloaded models in /app/models/ (container path)
# Models are downloaded via Setup Wizard or Models Page in Admin UI
# You can switch models at runtime via Dashboard without container restart

#LOCAL_STT_MODEL_PATH=/app/models/stt/vosk-model-en-us-0.22
#LOCAL_LLM_MODEL_PATH=/app/models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
#LOCAL_TTS_MODEL_PATH=/app/models/tts/en_US-lessac-medium.onnx

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - LLM Performance Tuning
# ───────────────────────────────────────────────────────────────────────────
# Tune these for your hardware. Defaults are optimized for 4-8 core CPUs.
# Higher values = better quality but slower inference

#LOCAL_LLM_THREADS=16          # CPU threads for inference (default: min(16, cpu_count))
#LOCAL_LLM_CONTEXT=768         # Context window size (lower = faster, 512-2048)
#LOCAL_LLM_BATCH=256           # Batch size for prompt processing (128-512)
#LOCAL_LLM_MAX_TOKENS=48       # Max tokens per response (32-128 for voice)
#LOCAL_LLM_TEMPERATURE=0.2     # Response creativity (0.1-0.5 for consistency)
#LOCAL_LLM_TOP_P=0.85          # Nucleus sampling (0.8-0.95)
#LOCAL_LLM_REPEAT_PENALTY=1.05 # Repetition penalty (1.0-1.2)
#LOCAL_LLM_USE_MLOCK=0         # Lock model in RAM (1=yes, requires privileges)
#LOCAL_LLM_INFER_TIMEOUT_SEC=30 # Max seconds for LLM inference

# ───────────────────────────────────────────────────────────────────────────
# Local AI Server - GPU Acceleration (NVIDIA CUDA)
# ───────────────────────────────────────────────────────────────────────────
# Offload LLM layers to GPU for faster inference (requires NVIDIA GPU + CUDA)
# Also requires uncommenting GPU deploy config in docker-compose.yml

LOCAL_LLM_GPU_LAYERS=0        # GPU layer offloading:
                              #   0  = CPU only (default, no GPU required)
                              #   -1 = Auto-detect (use GPU if CUDA available)
                              #   N  = Offload N layers to GPU (e.g., 35)
                              # Tip: Start with 35 layers, adjust based on VRAM

# To enable GPU support:
# 1. Install NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/
# 2. Set LOCAL_LLM_GPU_LAYERS=-1 or specific layer count
# 3. Uncomment GPU deploy section in docker-compose.yml
# 4. Rebuild: docker compose build local-ai-server

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL: Monitoring & Email (SECRETS)
# ═══════════════════════════════════════════════════════════════════════════
# Get API key at https://resend.com
# Configure email recipients in config/ai-agent.yaml under monitoring.email

RESEND_API_KEY=

# ═══════════════════════════════════════════════════════════════════════════
# OPTIONAL: Health Endpoint (Environment-Specific)
# ═══════════════════════════════════════════════════════════════════════════
# For external monitoring tools (Prometheus, etc.)

# HEALTH_BIND_HOST=127.0.0.1  # Use 0.0.0.0 for remote monitoring
# HEALTH_BIND_PORT=15000

# ═══════════════════════════════════════════════════════════════════════════
# DIAGNOSTIC: Audio Debugging (Troubleshooting Only)
# ═══════════════════════════════════════════════════════════════════════════
# DO NOT enable in production - creates WAV file taps for analysis
# See docs/TROUBLESHOOTING.md for usage

DIAG_ENABLE_TAPS=false
# DIAG_TAP_PRE_SECS=1
# DIAG_TAP_POST_SECS=1
# DIAG_TAP_OUTPUT_DIR=/tmp/ai-engine-taps
# DIAG_EGRESS_SWAP_MODE=none
# DIAG_EGRESS_FORCE_MULAW=false
# DIAG_ATTACK_MS=0

# ═══════════════════════════════════════════════════════════════════════════
# For Application Behavior Configuration, edit config/ai-agent.yaml:
# ═══════════════════════════════════════════════════════════════════════════
# ✅ Audio transport mode (audiosocket vs externalmedia)
# ✅ Downstream playback mode (stream vs file)
# ✅ Pipelines and providers
# ✅ Barge-in settings
# ✅ VAD configuration
# ✅ AudioSocket/ExternalMedia settings
#
# Advanced environment overrides (optional):
# - AUDIO_TRANSPORT           # Override audio_transport from YAML
# - DOWNSTREAM_MODE           # Override downstream_mode from YAML
# - AUDIOSOCKET_HOST          # Override audiosocket.host
# - AUDIOSOCKET_PORT          # Override audiosocket.port
# - AUDIOSOCKET_FORMAT        # Override audiosocket.format
# - EXTERNAL_MEDIA_RTP_HOST   # Override external_media.rtp_host
# - AST_MEDIA_DIR             # Override fallback media directory for generated audio
# - ASTERISK_GID              # GID of asterisk group on host (default: 995) - auto-detected by preflight.sh
#                             # Used at build time to add container user to asterisk group
#                             # Run `id asterisk` to find your system's GID
#
# Restart after changing .env: docker-compose down && docker-compose up -d
# Restart after changing YAML: docker-compose restart ai-engine
